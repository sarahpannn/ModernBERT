{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "import sys\n",
    "import src.evals.data as data_module\n",
    "\n",
    "import torch\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = \"answerdotai/ModernBERT-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2488/2488 [00:00<00:00, 2717.78 examples/s]\n",
      "Map: 100%|██████████| 464/464 [00:00<00:00, 5194.92 examples/s]\n",
      "Map: 100%|██████████| 1431/1431 [00:00<00:00, 3830.42 examples/s]\n",
      "Map: 100%|██████████| 740/740 [00:00<00:00, 5002.58 examples/s]\n"
     ]
    }
   ],
   "source": [
    "chat = data_module.create_preference_to_flan_style_dataset(\n",
    "    task = \"sarahpann/rwb_chat\",\n",
    "    split = \"train\",\n",
    "    tokenizer_name = tokenizer,\n",
    "    max_seq_length = 3600,\n",
    "    prefix = \"Which response is the most helpful, relevant, and correct? \",\n",
    "\n",
    "    dataset_name=\"sarahpann/rwb_chat\",\n",
    "    dataset_subset=\"\",\n",
    "    task_column_names={\"sarahpann/rwb_chat\": ('chosen', 'rejected', 'og_dataset')}\n",
    ")\n",
    "\n",
    "chat_hard = data_module.create_preference_to_flan_style_dataset(\n",
    "    task = \"sarahpann/rwb_chat_hard\",\n",
    "    split = \"train\",\n",
    "    tokenizer_name = tokenizer,\n",
    "    max_seq_length = 3600,\n",
    "    prefix = \"Which response is the most helpful, relevant, and correct? \",\n",
    "\n",
    "    dataset_name=\"sarahpann/rwb_chat_hard\",\n",
    "    dataset_subset=\"\",\n",
    "    task_column_names={\"sarahpann/rwb_chat_hard\": ('chosen', 'rejected', 'og_dataset')}\n",
    ")\n",
    "\n",
    "reasoning = data_module.create_preference_to_flan_style_dataset(\n",
    "    task=\"sarahpann/rwb_reasoning\",\n",
    "    split='train',\n",
    "    tokenizer_name=tokenizer,\n",
    "    max_seq_length=3600,\n",
    "    prefix=\"Determine which response is the best choice based on mathematical or programming accuracy. \",\n",
    "\n",
    "    dataset_name=\"sarahpann/rwb_reasoning\",\n",
    "    dataset_subset=\"\",\n",
    "    task_column_names={\"sarahpann/rwb_reasoning\": ('chosen', 'rejected', 'og_dataset')}\n",
    ")\n",
    "\n",
    "safety = data_module.create_preference_to_flan_style_dataset(\n",
    "    task=\"sarahpann/rwb_safety\",\n",
    "    split='train',\n",
    "    tokenizer_name=tokenizer,\n",
    "    max_seq_length=3600,\n",
    "    prefix=\"Which response is the most helpful, relevant, and correct? \",\n",
    "\n",
    "    dataset_name=\"sarahpann/rwb_safety\",\n",
    "    dataset_subset=\"\",\n",
    "    task_column_names={\"sarahpann/rwb_safety\": ('chosen', 'rejected', 'og_dataset')}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForMaskedLM.from_pretrained(\"sarahpann/chat_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "chat_hard.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "reasoning.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "safety.set_format(type='torch', columns=['input_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_loader = torch.utils.data.DataLoader(chat, batch_size=1, shuffle=False)\n",
    "chat_hard_loader = torch.utils.data.DataLoader(chat_hard, batch_size=1, shuffle=False)\n",
    "reasoning_loader = torch.utils.data.DataLoader(reasoning, batch_size=1, shuffle=False)\n",
    "safety_loader = torch.utils.data.DataLoader(safety, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_tokenizer = AutoTokenizer.from_pretrained(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_dataset(model, subject_dataloader):\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    model.to(\"cuda\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for example in subject_dataloader:\n",
    "            input_ids = example['input_ids'].clone()\n",
    "            input_ids[:, -2] = real_tokenizer.mask_token_id\n",
    "            input_ids = input_ids.to(\"cuda\")\n",
    "            attention_mask = example['attention_mask'].to(\"cuda\")\n",
    "\n",
    "            output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            logits = output.logits\n",
    "            pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            if pred[0, -2] == example['input_ids'][0, -2]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "            \n",
    "    print(f\"Accuracy: {correct / total}\")\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m chat_acc \u001b[38;5;241m=\u001b[39m \u001b[43meval_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchat_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m chat_hard_acc \u001b[38;5;241m=\u001b[39m eval_dataset(model, chat_hard_loader)\n\u001b[1;32m      3\u001b[0m reasoning_acc \u001b[38;5;241m=\u001b[39m eval_dataset(model, reasoning_loader)\n",
      "Cell \u001b[0;32mIn[9], line 21\u001b[0m, in \u001b[0;36meval_dataset\u001b[0;34m(model, subject_dataloader)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m pred[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]:\n\u001b[1;32m     20\u001b[0m             correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 21\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39mtotal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m correct \u001b[38;5;241m/\u001b[39m total\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chat_acc = eval_dataset(model, chat_loader)\n",
    "chat_hard_acc = eval_dataset(model, chat_hard_loader)\n",
    "reasoning_acc = eval_dataset(model, reasoning_loader)\n",
    "safety_acc = eval_dataset(model, safety_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Mar 27 16:52:49 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.54.03              Driver Version: 535.54.03    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000001:00:00.0 Off |                    0 |\n",
      "| N/A   77C    P0             300W / 300W |  78657MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     74148      C   /home/azureuser/miniconda3/bin/python     73662MiB |\n",
      "|    0   N/A  N/A     77976      C   /home/azureuser/miniconda3/bin/python      4970MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chat': 0.7946141479099679, 'chat_hard': 0.2521551724137931, 'reasoning': 0.76659678546471, 'safety': 0.7918918918918919}\n"
     ]
    }
   ],
   "source": [
    "final_results = {\n",
    "    \"chat\": chat_acc,\n",
    "    \"chat_hard\": chat_hard_acc,\n",
    "    \"reasoning\": reasoning_acc,\n",
    "    \"safety\": safety_acc\n",
    "}\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
