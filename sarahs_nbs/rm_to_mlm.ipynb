{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Modeling to MLM Task\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Reward models are typically trained to output one scalar value for each state-action pair. In language modeling, they are typically decoder-only models with a classification head that predicts the reward for a given sequence. In this notebook, we will convert typical training data (that comes in the form of chosen and rejected pairs) and convert it into a finegrained, MLM task. Instead of chosen having the label value of \"1\" and rejected having the label value of \"0\", we will intersperse intermediate rewards throughout the sequence.\n",
    "\n",
    "## Method\n",
    "\n",
    "1. (Naive) We will first sprinkle the same reward value throughout the sequence. \n",
    "2. Try to evaluate each sentence via some automatic metric and use those as the individual rewards. (Likely OmegaPRM.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import Optional, cast, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "from src.flex_bert import *\n",
    "from src.evals.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbench_dataset = datasets.load_dataset(\"sarahpann/reward_bench_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset = datasets.load_dataset(\"sarahpann/processed_skywork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "213.62073004099162\n",
      "169.15850087839158\n"
     ]
    }
   ],
   "source": [
    "num_c_chars = 0\n",
    "\n",
    "for example in rbench_dataset['train']['chosen']:\n",
    "    split = example.split(\" \")\n",
    "    num_c_chars += len(split)\n",
    "\n",
    "num_inc_chars = 0\n",
    "\n",
    "for example in rbench_dataset['train']['rejected']:\n",
    "    split = example.split(\" \")\n",
    "    num_inc_chars += len(split)\n",
    "\n",
    "print(num_c_chars / len(rbench_dataset['train']['chosen']))\n",
    "print(num_inc_chars / len(rbench_dataset['train']['rejected']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "407.8271777707245\n",
      "439.8684392763367\n"
     ]
    }
   ],
   "source": [
    "num_c_chars = 0\n",
    "\n",
    "for example in original_dataset[\"train\"][\"chosen\"]:\n",
    "    split = example.split(\" \")\n",
    "    num_c_chars += len(split)\n",
    "\n",
    "num_inc_chars = 0\n",
    "\n",
    "for example in original_dataset[\"train\"][\"rejected\"]:\n",
    "    split = example.split(\" \")\n",
    "    num_inc_chars += len(split)\n",
    "\n",
    "print(num_c_chars / len(original_dataset[\"train\"][\"chosen\"]))\n",
    "print(num_inc_chars / len(original_dataset[\"train\"][\"rejected\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average length of a chosen sequence is 407 and 439 for rejected ones. This makes for 4 and 4 intermediate rewards respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sprinkle_the_same_label(example, freq=100):\n",
    "    \"\"\"\n",
    "    Insert [CLS] then tokenize.\n",
    "\n",
    "    The number of labels should scale up based on 100 words. If the text < 100 \n",
    "    words, then the number of labels should be 1.\n",
    "    \"\"\"\n",
    "    chosen = example[\"chosen\"].split(\" \")\n",
    "    rejected = example[\"rejected\"].split(\" \")\n",
    "\n",
    "    if len(chosen) < freq:\n",
    "        chosen = chosen + [\"[CLS]\"]\n",
    "    else:\n",
    "        for i in range(freq, len(chosen), freq):\n",
    "            chosen = chosen[:i] + [\"[CLS]\"] + chosen[i:]\n",
    "\n",
    "    if len(rejected) < freq:\n",
    "        rejected = rejected + [\"[CLS]\"]\n",
    "    else:\n",
    "        for i in range(freq, len(rejected), freq):\n",
    "            rejected = rejected[:i] + [\"[CLS]\"] + rejected[i:]\n",
    "\n",
    "    return {\"chosen_labeled\": \" \".join(chosen), \n",
    "            \"rejected_labeled\": \" \".join(rejected),\n",
    "            \"num_chosen_labels\": len(chosen) // freq,\n",
    "            \"num_rejected_labels\": len(rejected) // freq}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected', 'og_dataset', 'chosen_labeled', 'rejected_labeled', 'num_chosen_labels', 'num_rejected_labels'],\n",
       "        num_rows: 5123\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbench_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5123/5123 [00:00<00:00, 6902.48 examples/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 46.07ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sarahpann/reward_bench_processed_labeled/commit/08c8500db2e4faed516a0bc2a7a48ac9cc7a6579', commit_message='Upload dataset', commit_description='', oid='08c8500db2e4faed516a0bc2a7a48ac9cc7a6579', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbench_dataset['train'] = rbench_dataset['train'].map(sprinkle_the_same_label)\n",
    "\n",
    "rbench_dataset.push_to_hub(\"sarahpann/reward_bench_processed_labeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected', 'og_dataset', 'chosen_labeled', 'rejected_labeled', 'num_chosen_labels', 'num_rejected_labels'],\n",
       "        num_rows: 5123\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbench_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset['train'] = original_dataset['train'].map(sprinkle_the_same_label)\n",
    "original_dataset['test'] = original_dataset['test'].map(sprinkle_the_same_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 35/35 [00:02<00:00, 16.14ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 35/35 [00:02<00:00, 16.30ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 2/2 [00:14<00:00,  7.35s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 14.84ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.84s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sarahpann/processed_skywork_labeled/commit/55f23c17a24fa57d366a8539443f9d981ec0a373', commit_message='Upload dataset', commit_description='', oid='55f23c17a24fa57d366a8539443f9d981ec0a373', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset.push_to_hub(\"sarahpann/processed_skywork_labeled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['chosen', 'rejected', 'chosen_labeled', 'rejected_labeled', 'num_chosen_labels', 'num_rejected_labels'],\n",
       "        num_rows: 69314\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['chosen', 'rejected', 'chosen_labeled', 'rejected_labeled', 'num_chosen_labels', 'num_rejected_labels'],\n",
       "        num_rows: 7702\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bclavie/olmo_bert_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_process_ds(examples, tokenizer):\n",
    "    tokenized_chosen = tokenizer(examples[\"chosen_labeled\"])\n",
    "    tokenized_rejected = tokenizer(examples[\"rejected_labeled\"])\n",
    "\n",
    "    chosen_labels = [[-100] * len(example) for example in tokenized_chosen[\"input_ids\"]]\n",
    "    rejected_labels = [[-100] * len(example) for example in tokenized_rejected[\"input_ids\"]]\n",
    "\n",
    "    cls_token = 50281\n",
    "\n",
    "                       \n",
    "    for i, example in enumerate(tokenized_chosen[\"input_ids\"]):\n",
    "        for j in range(len(example)):\n",
    "            if example[j] == cls_token:\n",
    "                chosen_labels[i][j] = 1\n",
    "    \n",
    "    for i, example in enumerate(tokenized_rejected[\"input_ids\"]):\n",
    "        for j in range(len(example)):\n",
    "            if example[j] == cls_token:\n",
    "                rejected_labels[i][j] = 0\n",
    "\n",
    "    return {\"input_ids\": tokenized_chosen[\"input_ids\"] + tokenized_rejected[\"input_ids\"], \n",
    "            \"attention_mask\": tokenized_chosen[\"attention_mask\"] + tokenized_rejected[\"attention_mask\"],\n",
    "            \"labels\": chosen_labels + rejected_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 69314/69314 [03:46<00:00, 305.64 examples/s]\n",
      "Map: 100%|██████████| 7702/7702 [00:25<00:00, 306.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rm_columns = [\"chosen\", \"rejected\", \"chosen_labeled\", \"rejected_labeled\", \"num_chosen_labels\", \"num_rejected_labels\"]\n",
    "\n",
    "\n",
    "tokenized_train_ds = original_dataset['train'].map(lambda x: tokenize_and_process_ds(x, tokenizer), batched=True, remove_columns=rm_columns)\n",
    "tokenized_test_ds = original_dataset['test'].map(lambda x: tokenize_and_process_ds(x, tokenizer), batched=True, remove_columns=rm_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to verify that function quickly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 206.53 examples/s]\n",
      "Map: 100%|██████████| 50/50 [00:00<00:00, 208.23 examples/s]\n"
     ]
    }
   ],
   "source": [
    "rm_columns = [\"chosen\", \"rejected\", \"chosen_labeled\", \"rejected_labeled\", \"num_chosen_labels\", \"num_rejected_labels\"]\n",
    "\n",
    "\n",
    "mini_tokenized_train_ds = original_dataset['train'].select(range(50))\n",
    "mini_tokenized_test_ds = original_dataset['test'].select(range(50))\n",
    "\n",
    "mini_tokenized_train = mini_tokenized_train_ds.map(lambda x: tokenize_and_process_ds(x, tokenizer), batched=True, remove_columns=rm_columns)\n",
    "mini_tokenized_test = mini_tokenized_test_ds.map(lambda x: tokenize_and_process_ds(x, tokenizer), batched=True, remove_columns=rm_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (2/2 shards): 100%|██████████| 138628/138628 [00:00<00:00, 273630.82 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 15404/15404 [00:00<00:00, 277124.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "datasets.Dataset.save_to_disk(tokenized_train_ds, \"/home/public/span/MATH_DPO/modern_bert_test/bert24/data/train\")\n",
    "datasets.Dataset.save_to_disk(tokenized_test_ds, \"/home/public/span/MATH_DPO/modern_bert_test/bert24/data/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_ds = datasets.load_from_disk(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/data/train\")\n",
    "tokenized_test_ds = datasets.load_from_disk(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/data/val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_prefix_in_state_dict_if_present(\n",
    "    state_dict, prefix\n",
    "):\n",
    "    r\"\"\"Strip the prefix in state_dict in place, if any.\n",
    "\n",
    "    ..note::\n",
    "        Given a `state_dict` from a DP/DDP model, a local model can load it by applying\n",
    "        `consume_prefix_in_state_dict_if_present(state_dict, \"module.\")` before calling\n",
    "        :meth:`torch.nn.Module.load_state_dict`.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): a state-dict to be loaded to the model.\n",
    "        prefix (str): prefix.\n",
    "    \"\"\"\n",
    "    keys = sorted(state_dict.keys())\n",
    "    for key in keys:\n",
    "        if key.startswith(prefix):\n",
    "            newkey = key[len(prefix) :]\n",
    "            state_dict[newkey] = state_dict.pop(key)\n",
    "\n",
    "    # also strip the prefix in metadata if any.\n",
    "    if \"_metadata\" in state_dict:\n",
    "        metadata = state_dict[\"_metadata\"]\n",
    "        for key in list(metadata.keys()):\n",
    "            # for the metadata dict, the key can be:\n",
    "            # '': for the DDP module, which we want to remove.\n",
    "            # 'module': for the actual model.\n",
    "            # 'module.xx.xx': for the rest.\n",
    "\n",
    "            if len(key) == 0:\n",
    "                continue\n",
    "            newkey = key[len(prefix) :]\n",
    "            metadata[newkey] = metadata.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_state_dict = \"\"\n",
    "new_state_dict = \"\"\n",
    "\n",
    "state_dict = torch.load(original_state_dict)['state']['model']\n",
    "consume_prefix_in_state_dict_if_present(state_dict, \"model.\")\n",
    "torch.save(state_dict, new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/yamls/test/sequence_classification_og.yaml\") as f:\n",
    "    yaml_config = om.load(f)\n",
    "\n",
    "cfg = cast(DictConfig, yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_flex_bert_classification(\n",
    "    num_labels=cfg.model.num_labels,\n",
    "    pretrained_checkpoint=cfg.model.pretrained_checkpoint,\n",
    "    model_config=cfg.model.model_config,\n",
    "    tokenizer_name=cfg.tokenizer_name,\n",
    "    token_classification=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.AutoModelForTokenClassification.from_pretrained(\"google-bert/bert-base-uncased\", num_labels=2)\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "tokenized_test_ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_tokenized_train.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "mini_tokenized_test.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  101,  1026,  1064,  4088,  1035,  1997,  1035,  3793,  1064,  1028,\n",
      "         1026,  1064,  2707,  1035, 20346,  1035,  8909,  1064,  1028,  2291,\n",
      "         1026,  1064,  2203,  1035, 20346,  1035,  8909,  1064,  1028,  6276,\n",
      "         3716,  3058,  1024,  2285, 16798,  2509,  2651,  3058,  1024,  2656,\n",
      "        21650, 16798,  2549,  1026,  1064,  1041,  4140,  1035,  8909,  1064,\n",
      "         1028,  1026,  1064,  2707,  1035, 20346,  1035,  8909,  1064,  1028,\n",
      "         5310,  1026,  1064,  2203,  1035, 20346,  1035,  8909,  1064,  1028,\n",
      "         2071,  2017,  4339,  2019,  5385,  1011,  2773, 10061,  2006,  2129,\n",
      "         8494,  2013, 29530,  2047,  2259,  2003,  1037,  5957,  2083,  2029,\n",
      "         2000,  2228,  2055, 19483,  8474,  1029,  1026,  1064,  1041,  4140,\n",
      "         1035,  8909,  1064,  1028,  1026,  1064,  2707,  1035, 20346,  1035,\n",
      "         8909,  1064,  1028,  3353,  1026,  1064,  2203,  1035, 20346,  1035,\n",
      "         8909,  1064,  1028,  5121,   999,  2182,  2003,  2019,  5385,  1011,\n",
      "         2773, 10061,  2006,  2129,  8494,  2013, 29530,  2047,  2259,  2003,\n",
      "         1037,  5957,  2083,  2029,  2000,  2228,  2055, 19483,  8474,  1024,\n",
      "         8494,  2003,  1037,  5957,  2008,  2003,  2411, 17092,  1998,  2104,\n",
      "        10175,  5657,  2094,  1010,  2664,  2009,  4324,  1037,  2307,  3066,\n",
      "         1997,  7784,  2005,  4824,  1996,  3375,  6550,  2090,  5907,  1010,\n",
      "        13798,  1010,  1998,  1996,  4044,  1012,  1999,  2023,  9491,  1010,\n",
      "         1045,  7475,  2008,  8494,  4240,  2004,  1037,  3928,  2609,  2005,\n",
      "         3241,  2055, 19483,  8474,  2107,  2004,   101,  8331,  3012,  1010,\n",
      "        27637,  1010,  1998,  8651,  1012,  2034,  1010,  8494,  2003,  1037,\n",
      "         5957,  2008,  2003,  7356,  2011,  8331,  3012,  1998, 27637,  1012,\n",
      "         2009,  2003,  1037,  9415,  2008,  6526,  1999,  1037,  2110,  1997,\n",
      "         5377,  2689,  1010,  9564,  2013,  5024,  2000,  6381,  1998,  2067,\n",
      "         2153,  5834,  2006,  1996,  3785,  1012,  2023,  8331,  3012,  7860,\n",
      "         3151,  8026, 12086,  2107,  2004,  3287,  1013,  2931,  1010,  5637,\n",
      "         1013,  3442,  1010,  1998,  3267,  1013,  3226,  1010,  1998, 18675,\n",
      "         2149,  2000,  2228,  2055,  5907,  1998, 13798,  2004,  8331,  1998,\n",
      "         2330,  2000,  7613,  1012,  2117,  1010,  8494,  2003,  1037,  5957,\n",
      "         2008,  2003,  7356,  2011,  1037,  3168,  1997, 18525, 13290,  3012,\n",
      "         1012,  2009,  2003,  1037,  2686,  2008,  6526,  2090,  2048,  2163,\n",
      "         1010,  1037,  2173,  1997,  6653,  1998,  8651,  1012,  2023, 18525,\n",
      "        13290,  3012,  2003,  7686,  1999,  1996,  6322,   101,  1997,  2116,\n",
      "        19483,  2111,  1010,  2040,  2411,  4839,  1999,  1037,  2110,  1997,\n",
      "         1999,  1011,  2090,  2791,  1010,  4445,  3929, 11414,  2046,  3151,\n",
      "         5907,  1998,  4424,  7236,  4496,  3110,  6625,  1999,  1996,  7731,\n",
      "         3226,  1012,  2353,  1010,  8494,  2003,  1037,  5957,  2008,  2003,\n",
      "         2411,  3378,  2007,  6900,  1998, 10796,  1012,  2023,  4997,  2523,\n",
      "        11138,  1996,  3971,  1999,  2029, 19483,  2791,  2003,  2411,  2464,\n",
      "         2004,  6530,  2030, 17727,  5397,  2011,  7731,  3226,  1012,  2174,\n",
      "         1010,  2011, 24104,  2075,  1996,  8494,  2004,  1037,  2609,  1997,\n",
      "        19483,  6061,  1010,  2057,  2064,  4119,  2122,  4997, 22807,  1998,\n",
      "         3443,  2047, 22143,  2008,  8439,  1996,  8906,  1998,  4138,  2791,\n",
      "         1997, 19483,  2166,  1012,  2000,  8849,  1996,  4022,  1997,  8494,\n",
      "         2004,  1037,  5957,  2005,  3241,   101,  2055, 19483,  8474,  1010,\n",
      "         1045,  2735,  2000,  1996,  2147,  1997,  2195,  3324,  1998,  4898,\n",
      "         2040,  2031,  5117,  2007,  2023,  3430,  1999,  4310,  1998,  2245,\n",
      "         1011,  4013, 22776,  3971,  1012,  2028,  2107,  3063,  2003,  5586,\n",
      "        10514,  4757,  2386,  1010,  2040,  2038,  2580,  1037,  2186,  1997,\n",
      "         7008,  2008, 17120,  8494,  2004,  1037,  2686,  1997,  6061,  1998,\n",
      "         8651,  1012,  1999,  2014,  2147,  1010, 10514,  4757,  2386,  7860,\n",
      "         3151, 21951,  1997,  5907,  1998, 13798,  2011,  4526,  4871,  2008,\n",
      "        17120,  8494,  2004,  1037,  2686,  1997,  8331,  3012,  1998, 27637,\n",
      "         1012,  2178,  3063,  2040,  2038,  5117,  2007,  8494,  2004,  1037,\n",
      "         5957,  2005,  3241,  2055, 19483,  8474,  2003,  9465,  5622,  7446,\n",
      "         1010,  2040,  2038,  2580,  1037,  2186,  1997,  2573,  2008,  8849,\n",
      "         1996,  3971,   101,  1999,  2029,  2679,  1998,  5907, 29261,  1999,\n",
      "         2137,  3226,  1012,  1999,  2028,  1997,  2010,  2573,  1010,  4159,\n",
      "         1000,  8494,  1010,  1000,  5622,  7446,  3594,  8494,  2000,  3443,\n",
      "         1037,  2186,  1997, 10061, 10466,  2008, 23408, 11045,  2119,  1996,\n",
      "         5053,  1998,  1996, 24083,  1997,  1996,  3019,  2088,  1012,  2011,\n",
      "         2478,  8494,  2004,  1037,  5396,  1010,  5622,  7446,  7860,  3151,\n",
      "        21951,  1997,  5907,  1998, 13798,  1010,  1998,  9005,  1037,  2686,\n",
      "         2005,  2047,  3596,  1997,  3670,  1998,  4767,  1012,  1999,  2804,\n",
      "         2000,  1996,  2147,  1997,  2122,  3324,  1010,  1045,  2036,  4009,\n",
      "         2006,  1996,  7896,  1997,  2195,  5784,  2040,  2031,  5117,  2007,\n",
      "         8494,  2004,  1037,  5957,  2005,  3241,  2055, 19483,  8474,  1012,\n",
      "         2028,  2107,  6288,  2003,  7354, 10208,  1010,  2040,   101,  2038,\n",
      "         2517,  2055,  1996,  3971,  1999,  2029,  8494,  2064,  2022,  2464,\n",
      "         2004,  1037,  2686,  1997,  6061,  1998,  8651,  1012,  1999,  2014,\n",
      "         2338,  1000,  1996,  3451,  4331,  1997,  7603,  1010,  1000, 10208,\n",
      "         9251,  2008,  8494,  2064,  2022,  2464,  2004,  1037,  2686,  2073,\n",
      "         6699,  2024,  2489,  2000,  4834,  1998, 10938,  1010,  1998,  2073,\n",
      "         3151,  8026, 12086,  2107,  2004,  3287,  1013,  2931,  1998,  5637,\n",
      "         1013,  3442,  2024,  8315,  1012,  2178,  6288,  2040,  2038,  5117,\n",
      "         2007,  8494,  2004,  1037,  5957,  2005,  3241,  2055, 19483,  8474,\n",
      "         2003, 12924,  7055,  1010,  2040,  2038,  2517,  2055,  1996,  3971,\n",
      "         1999,  2029,  5907,  1998, 13798,  2024,  4685,  8082,  4490,  2008,\n",
      "         2024,  7887,  2108,  2128,  1011,  2580,  1998,  2128,  1011, 10009,\n",
      "         1012,  1999,  2014,  2338,  1000,  5907,  4390,  1010,  1000,  7055,\n",
      "          101,  9251,  2008,  5907,  1998, 13798,  2024,  2025,  4964,  7236,\n",
      "         1010,  2021,  2738,  2024,  2330,  2000,  7613,  1998,  2689,  1012,\n",
      "         2011,  2478,  8494,  2004,  1037,  5957,  2005,  3241,  2055, 19483,\n",
      "         8474,  1010,  2057,  2064,  3443,  2047,  3596,  1997,  3670,  1998,\n",
      "         4767,  2008,  4119,  3151,  8026, 12086,  1998,  3443,  2047, 12020,\n",
      "         2005,  5907,  1998,  4424,  3670,  1012,  1999,  7091,  1010,  8494,\n",
      "         2003,  1037,  5957,  2008,  4324,  2307,  4022,  2005,  3241,  2055,\n",
      "        19483,  8474,  2107,  2004,  8331,  3012,  1010, 27637,  1010,  1998,\n",
      "         8651,  1012,  2011, 11973,  2007,  2023,  3430,  1999,  4310,  1998,\n",
      "         2245,  1011,  4013, 22776,  3971,  1010,  3324,  1010,  4898,  1010,\n",
      "         1998,  5784,  2064,  3443,  2047, 22143,  2008,  4119,  3151,  8026,\n",
      "        12086,  1998,  3443,  2047, 12020,  2005,  5907,  1998,   101,  4424,\n",
      "         3670,  1012,  2011, 24104,  2075,  1996,  8494,  2004,  1037,  2609,\n",
      "         1997, 19483,  6061,  1010,  2057,  2064,  3443,  1037,  2062,  7578,\n",
      "         1998, 18678,  2088,  2008, 21566,  1996,  4138,  2791,  1998,  8906,\n",
      "         1997,  2529,  3325,  1012,  1026,  1064,  1041,  4140,  1035,  8909,\n",
      "         1064,  1028,   102])\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/temp/ipykernel_145312/2781556612.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0)[: , :512]\n",
      "/home/public/span/temp/ipykernel_145312/2781556612.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0)[:, :512]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TokenClassifierOutput(loss=None, logits=tensor([[[-0.0625,  0.1349],\n",
       "         [-0.0397,  0.3458],\n",
       "         [-0.3241,  0.6699],\n",
       "         ...,\n",
       "         [-0.1785, -0.1765],\n",
       "         [ 0.1751, -0.3413],\n",
       "         [-0.3007, -0.3201]]], grad_fn=<ViewBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = mini_tokenized_train[8]\n",
    "print(example['input_ids'])\n",
    "num_cls = sum([1 for i in example['input_ids'] if i == 101])\n",
    "print(num_cls)\n",
    "input_ids = torch.tensor(example[\"input_ids\"]).unsqueeze(0)[: , :512]\n",
    "attention_mask = torch.tensor(example[\"attention_mask\"]).unsqueeze(0)[:, :512]\n",
    "\n",
    "model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import Optional, cast, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "from src.flex_bert import *\n",
    "from src.evals.data import *\n",
    "\n",
    "with open(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/yamls/test/sequence_classification_og.yaml\") as f:\n",
    "    yaml_config = om.load(f)\n",
    "\n",
    "cfg = cast(DictConfig, yaml_config)\n",
    "\n",
    "model = create_flex_bert_classification(\n",
    "    num_labels=cfg.model.num_labels,\n",
    "    pretrained_checkpoint=cfg.model.pretrained_checkpoint,\n",
    "    model_config=cfg.model.model_config,\n",
    "    tokenizer_name=cfg.tokenizer_name,\n",
    "    token_classification=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
