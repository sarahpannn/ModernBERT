{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import Optional, cast, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "from src.flex_bert import *\n",
    "from src.evals.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_prefix_in_state_dict_if_present(\n",
    "    state_dict, prefix\n",
    "):\n",
    "    r\"\"\"Strip the prefix in state_dict in place, if any.\n",
    "\n",
    "    ..note::\n",
    "        Given a `state_dict` from a DP/DDP model, a local model can load it by applying\n",
    "        `consume_prefix_in_state_dict_if_present(state_dict, \"module.\")` before calling\n",
    "        :meth:`torch.nn.Module.load_state_dict`.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): a state-dict to be loaded to the model.\n",
    "        prefix (str): prefix.\n",
    "    \"\"\"\n",
    "    keys = sorted(state_dict.keys())\n",
    "    for key in keys:\n",
    "        if key.startswith(prefix):\n",
    "            newkey = key[len(prefix) :]\n",
    "            state_dict[newkey] = state_dict.pop(key)\n",
    "\n",
    "    # also strip the prefix in metadata if any.\n",
    "    if \"_metadata\" in state_dict:\n",
    "        metadata = state_dict[\"_metadata\"]\n",
    "        for key in list(metadata.keys()):\n",
    "            # for the metadata dict, the key can be:\n",
    "            # '': for the DDP module, which we want to remove.\n",
    "            # 'module': for the actual model.\n",
    "            # 'module.xx.xx': for the rest.\n",
    "\n",
    "            if len(key) == 0:\n",
    "                continue\n",
    "            newkey = key[len(prefix) :]\n",
    "            metadata[newkey] = metadata.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/temp/ipykernel_35250/1181012206.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/ep1-ba542-rank0.pt\")\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/ep1-ba542-rank0.pt\")\n",
    "state_dict = state_dict['state']['model']\n",
    "consume_prefix_in_state_dict_if_present(state_dict, \"model.\")\n",
    "torch.save(state_dict, \"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/correct_names.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/yamls/test/sequence_classification_og.yaml\") as f:\n",
    "    yaml_config = om.load(f)\n",
    "\n",
    "cfg = cast(DictConfig, yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/MATH_DPO/modern_bert_test/bert24/src/bert_layers/model.py:1300: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pretrained_checkpoint)\n"
     ]
    }
   ],
   "source": [
    "model = create_flex_bert_classification(\n",
    "    num_labels=cfg.model.num_labels,\n",
    "    pretrained_checkpoint=cfg.model.pretrained_checkpoint,\n",
    "    model_config=cfg.model.model_config,\n",
    "    tokenizer_name=cfg.tokenizer_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset(\"sarahpann/reward_bench_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dsets = {'llmbar-adver-manual', 'hep-python', 'refusals-dangerous', \n",
    "                 'mt-bench-med', 'refusals-offensive', 'alpacaeval-length', \n",
    "                 'llmbar-natural', 'alpacaeval-hard', 'mt-bench-easy', \n",
    "                 'mt-bench-hard', 'llmbar-adver-GPTOut', 'hep-rust', \n",
    "                 'hep-java', 'donotanswer', 'hep-js', 'llmbar-adver-GPTInst',\n",
    "                   'math-prm', 'hep-go', 'alpacaeval-easy', 'xstest-should-respond', \n",
    "                   'llmbar-adver-neighbor', 'hep-cpp', 'xstest-should-refuse'}\n",
    "\n",
    "categories = {\n",
    "    \"chat\": {'alpacaeval-easy', 'alpacaeval-length', 'alpacaeval-hard', \n",
    "                'mt-bench-easy', 'mt-bench-med'},\n",
    "    \"chat_hard\": {'mt-bench-hard', 'llmbar-natural', 'llmbar-adver-neighbor',\n",
    "                    'llmbar-adver-GPTInst', 'llmbar-adver-GPTOut', 'llmbar-adver-manual'},\n",
    "    \"safety\": {'refusals-dangerous', 'refusals-offensive', 'xstest-should-respond', \n",
    "                'xstest-should-refuse', 'xstest-should-respond', 'donotanswer'},\n",
    "    \"reasoning\": {'math-prm', 'hep-cpp', 'hep-go', 'hep-java', 'hep-js', 'hep-python', 'hep-rust'},\n",
    "}\n",
    "\n",
    "inverse_map = {v: k for k in categories for v in categories[k]}\n",
    "category_tallies = {k: 0 for k in categories.keys()}\n",
    "category_amounts = {k: 0 for k in categories.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': ['What are the names of some famous actors that started their careers on Broadway?\\nSeveral famous actors started their careers on Broadway before making it big in film and television. Here are a few notable examples:\\n\\n1. Sarah Jessica Parker - Before she was Carrie Bradshaw on \"Sex and the City,\" Sarah Jessica Parker was a Broadway star, having appeared in productions like \"Annie\" as a child.\\n\\n2. Meryl Streep - Meryl Streep\\'s early career included Broadway productions such as \"Trelawny of the \\'Wells\\'\" and \"A Memory of Two Mondays / 27 Wagons Full of Cotton.\"\\n\\n3. Hugh Jackman - Hugh Jackman won a Tony Award for his role in \"The Boy from Oz\" and has been known for his stage work as well as his film career.\\n\\n4. Sutton Foster - Known for her television role in \"Younger,\" Sutton Foster is also a Broadway legend with leading roles in shows like \"Thoroughly Modern Millie\" and \"Anything Goes.\"\\n\\n5. Kristen Bell - Before she was the voice of Anna in \"Frozen\" or the star of \"The Good Place,\" Kristen Bell appeared in Broadway\\'s \"The Adventures of Tom Sawyer\" and \"The Crucible.\"\\n\\n6. Audra McDonald - Audra McDonald is a renowned Broadway actress with a record-breaking number of Tony Awards. She\\'s starred in \"Ragtime,\" \"Carousel,\" \"Master Class,\" and more.\\n\\n7. Nathan Lane - Nathan Lane is a Broadway veteran known for his roles in \"The Producers,\" \"A Funny Thing Happened on the Way to the Forum,\" and \"Angels in America.\"\\n\\n8. Idina Menzel - Before \"Frozen\" and \"Wicked\" made her a household name, Idina Menzel started on Broadway in shows like \"Rent\" and \"Hair.\"\\n\\n9. Lin-Manuel Miranda - Before \"Hamilton\" and \"In the Heights\" became huge hits, Lin-Manuel Miranda was performing on Broadway, eventually becoming a celebrated writer and actor.\\n\\n10. Lea Michele - Prior to her role on \"Glee,\" Lea Michele was a young Broadway actress in shows like \"Les Misérables,\" \"Ragtime,\" and \"Spring Awakening.\"\\n\\nThese actors are just a few examples of the many performers who have transitioned from the Broadway stage to broader fame in the entertainment industry. Broadway often serves as a proving ground for talent, and many actors continue to return to the stage throughout their careers.'], 'rejected': ['What are the names of some famous actors that started their careers on Broadway?\\nSome famous actors that started their careers on Broadway include: Tom Hanks, Meryl Streep, Laurence Olivier, Christopher Walken, and Jeremy Irons.'], 'og_dataset': ['alpacaeval-easy']}\n"
     ]
    }
   ],
   "source": [
    "for ex in dataloader:\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bclavie/olmo_bert_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['chosen', 'rejected', 'og_dataset'])\n"
     ]
    }
   ],
   "source": [
    "for ex in dataloader:\n",
    "    print(ex.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mt-bench-med': 'chat',\n",
       " 'mt-bench-easy': 'chat',\n",
       " 'alpacaeval-length': 'chat',\n",
       " 'alpacaeval-hard': 'chat',\n",
       " 'alpacaeval-easy': 'chat',\n",
       " 'llmbar-adver-neighbor': 'chat_hard',\n",
       " 'mt-bench-hard': 'chat_hard',\n",
       " 'llmbar-adver-GPTOut': 'chat_hard',\n",
       " 'llmbar-natural': 'chat_hard',\n",
       " 'llmbar-adver-GPTInst': 'chat_hard',\n",
       " 'llmbar-adver-manual': 'chat_hard',\n",
       " 'refusals-dangerous': 'safety',\n",
       " 'xstest-should-respond': 'safety',\n",
       " 'refusals-offensive': 'safety',\n",
       " 'donotanswer': 'safety',\n",
       " 'xstest-should-refuse': 'safety',\n",
       " 'hep-java': 'reasoning',\n",
       " 'hep-rust': 'reasoning',\n",
       " 'hep-cpp': 'reasoning',\n",
       " 'hep-go': 'reasoning',\n",
       " 'math-prm': 'reasoning',\n",
       " 'hep-js': 'reasoning',\n",
       " 'hep-python': 'reasoning'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_map['']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_tallies={'chat': 2646, 'chat_hard': 446, 'safety': 664, 'reasoning': 1428}\n",
      "accuracy_dict={'chat': 0.5317524115755627, 'chat_hard': 0.48060344827586204, 'safety': 0.4486486486486487, 'reasoning': 0.4989517819706499}\n",
      "total_accuracy=0.5059535428459887\n"
     ]
    }
   ],
   "source": [
    "# model = model.model\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in dataloader:\n",
    "        tokenized_chosen = tokenizer(example['chosen'], return_tensors=\"pt\")\n",
    "        tokenized_rejected = tokenizer(example['rejected'], return_tensors=\"pt\")\n",
    "\n",
    "        chosen_out = model(tokenized_chosen['input_ids'].to(\"cuda\"))\n",
    "        rejected_out = model(tokenized_rejected['input_ids'].to(\"cuda\"))\n",
    "\n",
    "        chosen_pred = torch.argmax(chosen_out.logits, dim=1)\n",
    "        rejected_pred = torch.argmax(rejected_out.logits, dim=1)\n",
    "\n",
    "        if chosen_pred == 1:\n",
    "            category_tallies[inverse_map[example['og_dataset'][0]]] += 1\n",
    "\n",
    "        if rejected_pred == 0:\n",
    "            category_tallies[inverse_map[example['og_dataset'][0]]] += 1\n",
    "\n",
    "        category_amounts[inverse_map[example['og_dataset'][0]]] += 2\n",
    "\n",
    "\n",
    "print(f\"{category_tallies=}\")\n",
    "\n",
    "# get accuracies\n",
    "accuracy_dict = {k: category_tallies[k] / category_amounts[k] for k in category_tallies.keys()}\n",
    "print(f\"{accuracy_dict=}\")\n",
    "\n",
    "# get total accuracy\n",
    "total_correct = sum(category_tallies.values())\n",
    "total_amount = sum(category_amounts.values())\n",
    "total_accuracy = total_correct / total_amount\n",
    "print(f\"{total_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
