{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01momegaconf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DictConfig\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01momegaconf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OmegaConf \u001b[38;5;28;01mas\u001b[39;00m om\n\u001b[0;32m---> 14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mflex_bert\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevals\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import Optional, cast, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "from src.flex_bert import *\n",
    "from src.evals.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_prefix_in_state_dict_if_present(\n",
    "    state_dict, prefix\n",
    "):\n",
    "    r\"\"\"Strip the prefix in state_dict in place, if any.\n",
    "\n",
    "    ..note::\n",
    "        Given a `state_dict` from a DP/DDP model, a local model can load it by applying\n",
    "        `consume_prefix_in_state_dict_if_present(state_dict, \"module.\")` before calling\n",
    "        :meth:`torch.nn.Module.load_state_dict`.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): a state-dict to be loaded to the model.\n",
    "        prefix (str): prefix.\n",
    "    \"\"\"\n",
    "    keys = sorted(state_dict.keys())\n",
    "    for key in keys:\n",
    "        if key.startswith(prefix):\n",
    "            newkey = key[len(prefix) :]\n",
    "            state_dict[newkey] = state_dict.pop(key)\n",
    "\n",
    "    # also strip the prefix in metadata if any.\n",
    "    if \"_metadata\" in state_dict:\n",
    "        metadata = state_dict[\"_metadata\"]\n",
    "        for key in list(metadata.keys()):\n",
    "            # for the metadata dict, the key can be:\n",
    "            # '': for the DDP module, which we want to remove.\n",
    "            # 'module': for the actual model.\n",
    "            # 'module.xx.xx': for the rest.\n",
    "\n",
    "            if len(key) == 0:\n",
    "                continue\n",
    "            newkey = key[len(prefix) :]\n",
    "            metadata[newkey] = metadata.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/temp/ipykernel_35250/1181012206.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/ep1-ba542-rank0.pt\")\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/ep1-ba542-rank0.pt\")\n",
    "state_dict = state_dict['state']['model']\n",
    "consume_prefix_in_state_dict_if_present(state_dict, \"model.\")\n",
    "torch.save(state_dict, \"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/correct_names.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/yamls/test/sequence_classification_og.yaml\") as f:\n",
    "    yaml_config = om.load(f)\n",
    "\n",
    "cfg = cast(DictConfig, yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/MATH_DPO/modern_bert_test/bert24/src/bert_layers/model.py:1300: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pretrained_checkpoint)\n"
     ]
    }
   ],
   "source": [
    "model = create_flex_bert_classification(\n",
    "    num_labels=cfg.model.num_labels,\n",
    "    pretrained_checkpoint=cfg.model.pretrained_checkpoint,\n",
    "    model_config=cfg.model.model_config,\n",
    "    tokenizer_name=cfg.tokenizer_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 354/354 [00:00<00:00, 1.55kB/s]\n",
      "Downloading data: 100%|██████████| 6.00M/6.00M [00:00<00:00, 11.3MB/s]\n",
      "Generating train split: 100%|██████████| 5123/5123 [00:00<00:00, 212578.35 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "ds = datasets.load_dataset(\"sarahpann/reward_bench_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-large\")\n",
    "model = transformers.AutoModelForMaskedLM.from_pretrained(\"sarahpann/all_at_once_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(ds['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dsets = {'llmbar-adver-manual', 'hep-python', 'refusals-dangerous', \n",
    "                 'mt-bench-med', 'refusals-offensive', 'alpacaeval-length', \n",
    "                 'llmbar-natural', 'alpacaeval-hard', 'mt-bench-easy', \n",
    "                 'mt-bench-hard', 'llmbar-adver-GPTOut', 'hep-rust', \n",
    "                 'hep-java', 'donotanswer', 'hep-js', 'llmbar-adver-GPTInst',\n",
    "                   'math-prm', 'hep-go', 'alpacaeval-easy', 'xstest-should-respond', \n",
    "                   'llmbar-adver-neighbor', 'hep-cpp', 'xstest-should-refuse'}\n",
    "\n",
    "categories = {\n",
    "    \"chat\": {'alpacaeval-easy', 'alpacaeval-length', 'alpacaeval-hard', \n",
    "                'mt-bench-easy', 'mt-bench-med'},\n",
    "    \"chat_hard\": {'mt-bench-hard', 'llmbar-natural', 'llmbar-adver-neighbor',\n",
    "                    'llmbar-adver-GPTInst', 'llmbar-adver-GPTOut', 'llmbar-adver-manual'},\n",
    "    \"safety\": {'refusals-dangerous', 'refusals-offensive', 'xstest-should-respond', \n",
    "                'xstest-should-refuse', 'xstest-should-respond', 'donotanswer'},\n",
    "    \"reasoning\": {'math-prm', 'hep-cpp', 'hep-go', 'hep-java', 'hep-js', 'hep-python', 'hep-rust'},\n",
    "}\n",
    "\n",
    "inverse_map = {v: k for k in categories for v in categories[k]}\n",
    "category_tallies = {k: 0 for k in categories.keys()}\n",
    "category_amounts = {k: 0 for k in categories.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chosen': ['What are the names of some famous actors that started their careers on Broadway?\\nSeveral famous actors started their careers on Broadway before making it big in film and television. Here are a few notable examples:\\n\\n1. Sarah Jessica Parker - Before she was Carrie Bradshaw on \"Sex and the City,\" Sarah Jessica Parker was a Broadway star, having appeared in productions like \"Annie\" as a child.\\n\\n2. Meryl Streep - Meryl Streep\\'s early career included Broadway productions such as \"Trelawny of the \\'Wells\\'\" and \"A Memory of Two Mondays / 27 Wagons Full of Cotton.\"\\n\\n3. Hugh Jackman - Hugh Jackman won a Tony Award for his role in \"The Boy from Oz\" and has been known for his stage work as well as his film career.\\n\\n4. Sutton Foster - Known for her television role in \"Younger,\" Sutton Foster is also a Broadway legend with leading roles in shows like \"Thoroughly Modern Millie\" and \"Anything Goes.\"\\n\\n5. Kristen Bell - Before she was the voice of Anna in \"Frozen\" or the star of \"The Good Place,\" Kristen Bell appeared in Broadway\\'s \"The Adventures of Tom Sawyer\" and \"The Crucible.\"\\n\\n6. Audra McDonald - Audra McDonald is a renowned Broadway actress with a record-breaking number of Tony Awards. She\\'s starred in \"Ragtime,\" \"Carousel,\" \"Master Class,\" and more.\\n\\n7. Nathan Lane - Nathan Lane is a Broadway veteran known for his roles in \"The Producers,\" \"A Funny Thing Happened on the Way to the Forum,\" and \"Angels in America.\"\\n\\n8. Idina Menzel - Before \"Frozen\" and \"Wicked\" made her a household name, Idina Menzel started on Broadway in shows like \"Rent\" and \"Hair.\"\\n\\n9. Lin-Manuel Miranda - Before \"Hamilton\" and \"In the Heights\" became huge hits, Lin-Manuel Miranda was performing on Broadway, eventually becoming a celebrated writer and actor.\\n\\n10. Lea Michele - Prior to her role on \"Glee,\" Lea Michele was a young Broadway actress in shows like \"Les Misérables,\" \"Ragtime,\" and \"Spring Awakening.\"\\n\\nThese actors are just a few examples of the many performers who have transitioned from the Broadway stage to broader fame in the entertainment industry. Broadway often serves as a proving ground for talent, and many actors continue to return to the stage throughout their careers.'], 'rejected': ['What are the names of some famous actors that started their careers on Broadway?\\nSome famous actors that started their careers on Broadway include: Tom Hanks, Meryl Streep, Laurence Olivier, Christopher Walken, and Jeremy Irons.'], 'og_dataset': ['alpacaeval-easy']}\n"
     ]
    }
   ],
   "source": [
    "for ex in dataloader:\n",
    "    print(ex)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['chosen', 'rejected', 'og_dataset'])\n"
     ]
    }
   ],
   "source": [
    "for ex in dataloader:\n",
    "    print(ex.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def tokenize_fn_factory(tokenizer, max_seq_length):\n",
    "        tokenized_postfix = tokenizer(\n",
    "            text=postfix,\n",
    "            padding=\"max_length\",\n",
    "            max_length=max_seq_length,\n",
    "            truncation=True,\n",
    "        )['input_ids'][1:]\n",
    "        len_tokenized_postfix = len(tokenized_postfix)\n",
    "\n",
    "        def tokenizer_fn(inp):\n",
    "            # flip a coin\n",
    "            coin = random.randint(0, 1)\n",
    "            # if coin == 0, then chosen goes first\n",
    "            choice_postfix = postfix + \" \" + str(coin)\n",
    "\n",
    "            if coin == 0:\n",
    "                pairs = [[prefix + \" Choice 0: \" + chosen, \" Choice 1: \" + rejected + choice_postfix] for chosen, rejected in zip(inp[\"chosen\"], inp[\"rejected\"])]\n",
    "            if coin == 1:\n",
    "                pairs = [[prefix + \" Choice 0: \" + rejected, \" Choice 1: \" + chosen + choice_postfix] for chosen, rejected in zip(inp[\"chosen\"], inp[\"rejected\"])]\n",
    "\n",
    "            tokenized_pairs = tokenizer(pairs,\n",
    "                                        max_length=max_seq_length, \n",
    "                                        truncation=True)\n",
    "\n",
    "            # Always has an answer even if truncated\n",
    "            if len(tokenized_pairs[\"input_ids\"]) == max_seq_length:\n",
    "                tokenized_pairs[\"input_ids\"] = tokenized_pairs[\"input_ids\"][:-(len_tokenized_postfix)] + tokenized_postfix\n",
    "\n",
    "            ret_dict = {\n",
    "                \"input_ids\": tokenized_pairs[\"input_ids\"],\n",
    "                \"token_type_ids\": tokenized_pairs[\"token_type_ids\"],\n",
    "                \"attention_mask\": tokenized_pairs[\"attention_mask\"],\n",
    "            }\n",
    "\n",
    "            return ret_dict\n",
    "        \n",
    "        return tokenizer_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'postfix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m columns_to_remove \u001b[38;5;241m=\u001b[39m [i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchosen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrejected\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mog_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m      3\u001b[0m ds \u001b[38;5;241m=\u001b[39m ds\u001b[38;5;241m.\u001b[39mmap(\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mtokenize_fn_factory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3600\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m      5\u001b[0m     batched\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# batched=False,\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     num_proc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m num_workers \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m num_workers,\n\u001b[1;32m      8\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,\n\u001b[1;32m      9\u001b[0m     remove_columns\u001b[38;5;241m=\u001b[39mcolumns_to_remove,\n\u001b[1;32m     10\u001b[0m     load_from_cache_file\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m, in \u001b[0;36mtokenize_fn_factory\u001b[0;34m(tokenizer, max_seq_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenize_fn_factory\u001b[39m(tokenizer, max_seq_length):\n\u001b[1;32m      2\u001b[0m     tokenized_postfix \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m----> 3\u001b[0m         text\u001b[38;5;241m=\u001b[39m\u001b[43mpostfix\u001b[49m,\n\u001b[1;32m      4\u001b[0m         padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mmax_seq_length,\n\u001b[1;32m      6\u001b[0m         truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      7\u001b[0m     )[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m      8\u001b[0m     len_tokenized_postfix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenized_postfix)\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtokenizer_fn\u001b[39m(inp):\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;66;03m# flip a coin\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'postfix' is not defined"
     ]
    }
   ],
   "source": [
    "columns_to_remove = [i for i in ['chosen', 'rejected', 'og_dataset'] if i is not None]\n",
    "\n",
    "ds = ds.map(\n",
    "    tokenize_fn_factory(tokenizer, 3600),\n",
    "    batched=True,\n",
    "    # batched=False,\n",
    "    num_proc=None if num_workers == 0 else num_workers,\n",
    "    batch_size=200,\n",
    "    remove_columns=columns_to_remove,\n",
    "    load_from_cache_file=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/azureuser/miniconda3/lib/python3.10/site-packages/torch/_inductor/compile_fx.py:124: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Boolean value of Tensor with more than one value is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m chosen_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(chosen_out\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     14\u001b[0m rejected_pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(rejected_out\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chosen_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     17\u001b[0m     category_tallies[inverse_map[example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mog_dataset\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]]] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rejected_pred \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Boolean value of Tensor with more than one value is ambiguous"
     ]
    }
   ],
   "source": [
    "# model = model.model\n",
    "model.to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for example in dataloader:\n",
    "        tokenized_chosen = tokenizer(example['chosen'], return_tensors=\"pt\")\n",
    "        tokenized_rejected = tokenizer(example['rejected'], return_tensors=\"pt\")\n",
    "\n",
    "        chosen_out = model(tokenized_chosen['input_ids'].to(\"cuda\"))\n",
    "        rejected_out = model(tokenized_rejected['input_ids'].to(\"cuda\"))\n",
    "\n",
    "        chosen_pred = torch.argmax(chosen_out.logits, dim=1)\n",
    "        rejected_pred = torch.argmax(rejected_out.logits, dim=1)\n",
    "\n",
    "        print(chosen_pred)\n",
    "        print(rejected_pred)\n",
    "\n",
    "        if chosen_pred == 1:\n",
    "            category_tallies[inverse_map[example['og_dataset'][0]]] += 1\n",
    "\n",
    "        if rejected_pred == 0:\n",
    "            category_tallies[inverse_map[example['og_dataset'][0]]] += 1\n",
    "\n",
    "        category_amounts[inverse_map[example['og_dataset'][0]]] += 2\n",
    "\n",
    "\n",
    "print(f\"{category_tallies=}\")\n",
    "\n",
    "# get accuracies\n",
    "accuracy_dict = {k: category_tallies[k] / category_amounts[k] for k in category_tallies.keys()}\n",
    "print(f\"{accuracy_dict=}\")\n",
    "\n",
    "# get total accuracy\n",
    "total_correct = sum(category_tallies.values())\n",
    "total_amount = sum(category_amounts.values())\n",
    "total_accuracy = total_correct / total_amount\n",
    "print(f\"{total_accuracy=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
