{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Callable, Dict, Iterator, List, Optional, Sequence, Union\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"sarahpann/processed_skywork\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bclavie/olmo_bert_template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50281, 17, 50282, 29, 93, 2043, 64, 1171, 64, 1156, 93, 2730, 93, 5478, 64, 10146, 64, 301, 49651, 10394, 29, 93, 423, 64, 10146, 64, 301, 49651, 187, 187, 28512, 1076, 28003, 10421, 27, 4565, 1384, 1508, 187, 14569, 10421, 27, 3436, 9218, 1384, 1348, 187, 187, 29, 93, 70, 302, 64, 301, 93, 2730, 93, 5478, 64, 10146, 64, 301, 49651, 4537, 29, 93, 423, 64, 10146, 64, 301, 49651, 187, 187, 34, 3817, 4428, 1027, 3295, 18098, 824, 347, 28580, 13, 8913, 24288, 13, 285, 41417, 15, 2615, 368, 2085, 247, 3410, 2127, 326, 9372, 253, 1180, 273, 18098, 285, 4648, 271, 2559, 1318, 323, 253, 4828, 323, 18098, 326, 2826, 625, 2223, 275, 253, 3817, 32, 29, 93, 70, 302, 64, 301, 93, 2730, 93, 5478, 64, 10146, 64, 301, 49651, 515, 5567, 29, 93, 423, 64, 10146, 64, 301, 49651, 187, 187, 424, 39, 5527, 27891, 342, 27021, 264, 45073, 424, 187, 4578, 43024, 187, 187, 30003, 310, 247, 13814, 2900, 970, 247, 19034, 281, 4657, 253, 9279, 9372, 285, 616, 9056, 13461, 15, 187, 187, 11202, 16659, 187, 4064, 18406, 1395, 4284, 8102, 187, 187, 2437, 43974, 27077, 27, 187, 50274, 1545, 4772, 4478, 21920, 1286, 2262, 187, 50270, 1286, 15, 37609, 64, 5560, 84, 426, 4284, 8102, 9, 565, 10, 187, 50270, 1286, 15, 42739, 426, 551, 187, 50266, 8, 19934, 5295, 337, 13, 187, 50266, 8, 5568, 3230, 5295, 374, 13, 187, 50266, 8, 72, 6182, 5295, 495, 187, 50270, 94, 535, 50274, 1545, 823, 64, 37609, 9, 1286, 13, 9279, 2262, 187, 50270, 32488, 4717, 247, 9279, 281, 253, 4828, 32488, 187, 50270, 1286, 15, 37609, 64, 5560, 84, 60, 37609, 62, 7079, 1881, 15, 42739, 15, 788, 9, 37609, 13, 337, 10, 535, 50274, 1545, 755, 64, 37609, 64, 5560, 84, 9, 1286, 2262, 187, 50270, 32488, 15968, 253, 17375, 9279, 9372, 32488, 187, 50270, 2309, 1881, 15, 37609, 64, 5560, 84, 187, 187, 4, 18466, 10393, 27, 187, 20285, 426, 43974, 27077, 1082, 187, 187, 4, 35794, 18098, 281, 253, 4828, 187, 71, 579, 953, 426, 14412, 19934, 1383, 686, 5568, 3230, 1383, 686, 19934, 1383, 686, 72, 6182, 1383, 686, 5568, 3230, 1383, 686, 5568, 3230, 6038, 187, 1542, 9279, 275, 18098, 27, 187, 50274, 20285, 15, 1911, 64, 37609, 9, 37609, 10, 187, 187, 4, 6724, 8279, 272, 9279, 9372, 187, 37609, 64, 5560, 84, 426, 4828, 15, 788, 64, 37609, 64, 5560, 84, 1082, 187, 1542, 9279, 13, 1385, 275, 9279, 64, 5560, 84, 15, 15565, 14850, 187, 50274, 3845, 9, 71, 3, 92, 37609, 17168, 551, 5560, 94, 2807, 187, 11202, 187, 187, 424, 1672, 45525, 424, 187, 187, 11, 50275, 1231, 4853, 247, 2634, 39, 5527, 27077, 65, 966, 342, 3082, 281, 823, 18098, 281, 253, 4828, 285, 19553, 253, 17375, 9279, 9372, 15, 187, 11, 50275, 510, 2634, 1911, 64, 37609, 65, 1332, 42344, 253, 9279, 1385, 275, 253, 2634, 37609, 64, 5560, 84, 65, 19034, 407, 253, 2801, 273, 253, 9279, 15, 187, 11, 50275, 510, 2634, 788, 64, 37609, 64, 5560, 84, 65, 1332, 6548, 253, 19034, 4508, 253, 17375, 9279, 9372, 15, 187, 11, 50275, 688, 253, 1650, 10393, 13, 359, 2794, 247, 2634, 39, 5527, 27077, 65, 4227, 13, 823, 247, 1618, 273, 18098, 281, 253, 4828, 13, 285, 3379, 253, 17375, 9279, 9372, 15, 187, 187, 424, 11021, 424, 187, 187, 11202, 187, 19934, 27, 374, 187, 5568, 3230, 27, 721, 187, 72, 6182, 27, 495, 187, 11202, 187, 187, 688, 436, 1650, 13, 253, 13461, 323, 1016, 9279, 403, 27, 187, 187, 11, 50275, 25989, 27, 337, 187, 11, 50275, 39419, 3230, 27, 374, 187, 11, 50275, 40, 6182, 27, 495, 187, 187, 510, 17375, 9372, 403, 5118, 407, 39763, 253, 1180, 273, 37102, 273, 1016, 9279, 407, 616, 9056, 13461, 17778, 93, 70, 302, 64, 301, 49651, 50282], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_chosen = \"\"'0[SEP]<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nA box contains different color fruits such as apples, bananas, and grapes. Can you provide a sample code that counts the number of fruits and uses an increased value for the counter for fruits that occur more often in the box?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n**Fruit Counter with Weighted Counts**\\n=====================================\\n\\nBelow is a Python solution using a dictionary to store the fruit counts and their respective weights.\\n\\n```python\\nfrom collections import defaultdict\\n\\nclass FruitCounter:\\n    def __init__(self):\\n        self.fruit_counts = defaultdict(int)\\n        self.weights = {\\n            \\'apple\\': 1,\\n            \\'banana\\': 2,\\n            \\'grape\\': 3\\n        }\\n\\n    def add_fruit(self, fruit):\\n        \"\"\"Add a fruit to the counter\"\"\"\\n        self.fruit_counts[fruit] += self.weights.get(fruit, 1)\\n\\n    def get_fruit_counts(self):\\n        \"\"\"Return the weighted fruit counts\"\"\"\\n        return self.fruit_counts\\n\\n# Example usage:\\ncounter = FruitCounter()\\n\\n# Adding fruits to the counter\\nfruits = [\\'apple\\', \\'banana\\', \\'apple\\', \\'grape\\', \\'banana\\', \\'banana\\']\\nfor fruit in fruits:\\n    counter.add_fruit(fruit)\\n\\n# Retrieving fruit counts\\nfruit_counts = counter.get_fruit_counts()\\nfor fruit, count in fruit_counts.items():\\n    print(f\"{fruit}: {count}\")\\n```\\n\\n**Explanation**\\n\\n*   We define a `FruitCounter` class with methods to add fruits to the counter and retrieve the weighted fruit counts.\\n*   The `add_fruit` method increments the fruit count in the `fruit_counts` dictionary by the weight of the fruit.\\n*   The `get_fruit_counts` method returns the dictionary containing the weighted fruit counts.\\n*   In the example usage, we create a `FruitCounter` instance, add a list of fruits to the counter, and print the weighted fruit counts.\\n\\n**Output**\\n\\n```\\napple: 2\\nbanana: 6\\ngrape: 3\\n```\\n\\nIn this example, the weights for each fruit are:\\n\\n*   Apple: 1\\n*   Banana: 2\\n*   Grape: 3\\n\\nThe weighted counts are calculated by multiplying the number of occurrences of each fruit by their respective weights.<|eot_id|>'\"\"\n",
    "tokenizer(first_chosen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that we want to reformat this classification problem into a MLM problem. We will append 0 or 1 to the beginning of a pair to indicate whether the first or second sequence is preferred. We will then train a model with varying masked probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [50281, 29, 93, 2043, 64, 1171, 64, 1156, 93, 2730, 93, 5478, 64, 10146, 64, 301, 49651, 10394, 29, 93, 423, 64, 10146, 64, 301, 49651, 187, 187, 28512, 1076, 28003, 10421, 27, 4565, 1384, 1508, 187, 14569, 10421, 27, 3436, 9218, 1384, 1348, 187, 187, 29, 93, 70, 302, 64, 301, 93, 2730, 93, 5478, 64, 10146, 64, 301, 49651, 4537, 29, 93, 423, 64, 10146, 64, 301, 49651, 187, 187, 34, 3817, 4428, 1027, 3295, 18098, 824, 347, 28580, 13, 8913, 24288, 13, 285, 41417, 15, 2615, 368, 2085, 247, 3410, 2127, 326, 9372, 253, 1180, 273, 18098, 285, 4648, 271, 2559, 1318, 323, 253, 4828, 323, 18098, 326, 2826, 625, 2223, 275, 253, 3817, 32, 29, 93, 70, 302, 64, 301, 93, 2730, 93, 5478, 64, 10146, 64, 301, 49651, 515, 5567, 29, 93, 423, 64, 10146, 64, 301, 49651, 187, 187, 424, 39, 5527, 27891, 342, 27021, 264, 45073, 424, 187, 4578, 43024, 187, 187, 30003, 310, 247, 13814, 2900, 970, 247, 19034, 281, 4657, 253, 9279, 9372, 285, 616, 9056, 13461, 15, 187, 187, 11202, 16659, 187, 4064, 18406, 1395, 4284, 8102, 187, 187, 2437, 43974, 27077, 27, 187, 50274, 1545, 4772, 4478, 21920, 1286, 2262, 187, 50270, 1286, 15, 37609, 64, 5560, 84, 426, 4284, 8102, 9, 565, 10, 187, 50270, 1286, 15, 42739, 426, 551, 187, 50266, 8, 19934, 5295, 337, 13, 187, 50266, 8, 5568, 3230, 5295, 374, 13, 187, 50266, 8, 72, 6182, 5295, 495, 187, 50270, 94, 535, 50274, 1545, 823, 64, 37609, 9, 1286, 13, 9279, 2262, 187, 50270, 32488, 4717, 247, 9279, 281, 253, 4828, 32488, 187, 50270, 1286, 15, 37609, 64, 5560, 84, 60, 37609, 62, 7079, 1881, 15, 42739, 15, 788, 9, 37609, 13, 337, 10, 535, 50274, 1545, 755, 64, 37609, 64, 5560, 84, 9, 1286, 2262, 187, 50270, 32488, 15968, 253, 17375, 9279, 9372, 32488, 187, 50270, 2309, 1881, 15, 37609, 64, 5560, 84, 187, 187, 4, 18466, 10393, 27, 187, 20285, 426, 43974, 27077, 1082, 187, 187, 4, 35794, 18098, 281, 253, 4828, 187, 71, 579, 953, 426, 14412, 19934, 1383, 686, 5568, 3230, 1383, 686, 19934, 1383, 686, 72, 6182, 1383, 686, 5568, 3230, 1383, 686, 5568, 3230, 6038, 187, 1542, 9279, 275, 18098, 27, 187, 50274, 20285, 15, 1911, 64, 37609, 9, 37609, 10, 187, 187, 4, 6724, 8279, 272, 9279, 9372, 187, 37609, 64, 5560, 84, 426, 4828, 15, 788, 64, 37609, 64, 5560, 84, 1082, 187, 1542, 9279, 13, 1385, 275, 9279, 64, 5560, 84, 15, 15565, 14850, 187, 50274, 3845, 9, 71, 3, 92, 37609, 17168, 551, 5560, 94, 2807, 187, 11202, 187, 187, 424, 1672, 45525, 424, 187, 187, 11, 50275, 1231, 4853, 247, 2634, 39, 5527, 27077, 65, 966, 342, 3082, 281, 823, 18098, 281, 253, 4828, 285, 19553, 253, 17375, 9279, 9372, 15, 187, 11, 50275, 510, 2634, 1911, 64, 37609, 65, 1332, 42344, 253, 9279, 1385, 275, 253, 2634, 37609, 64, 5560, 84, 65, 19034, 407, 253, 2801, 273, 253, 9279, 15, 187, 11, 50275, 510, 2634, 788, 64, 37609, 64, 5560, 84, 65, 1332, 6548, 253, 19034, 4508, 253, 17375, 9279, 9372, 15, 187, 11, 50275, 688, 253, 1650, 10393, 13, 359, 2794, 247, 2634, 39, 5527, 27077, 65, 4227, 13, 823, 247, 1618, 273, 18098, 281, 253, 4828, 13, 285, 3379, 253, 17375, 9279, 9372, 15, 187, 187, 424, 11021, 424, 187, 187, 11202, 187, 19934, 27, 374, 187, 5568, 3230, 27, 721, 187, 72, 6182, 27, 495, 187, 11202, 187, 187, 688, 436, 1650, 13, 253, 13461, 323, 1016, 9279, 403, 27, 187, 187, 11, 50275, 25989, 27, 337, 187, 11, 50275, 39419, 3230, 27, 374, 187, 11, 50275, 40, 6182, 27, 495, 187, 187, 510, 17375, 9372, 403, 5118, 407, 39763, 253, 1180, 273, 37102, 273, 1016, 9279, 407, 616, 9056, 13461, 17778, 93, 70, 302, 64, 301, 49651, 50282], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(ds['train'][0]['chosen'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/50 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 50/50 [00:00<00:00, 238.54 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_preprocess(ex):\n",
    "    new_ex = \"0[SEP]\" + ex['chosen']\n",
    "    return tokenizer(new_ex)\n",
    "\n",
    "rm_columns = [col for col in ds['train'].column_names]\n",
    "\n",
    "ds_train = ds['train'].select(range(50)).map(tokenize_and_preprocess, \n",
    "                                             batched=False,\n",
    "                                             remove_columns=rm_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"sarahpann/mlm_cls_skywork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"sarahpann/reward_bench_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50281\n",
      "tensor([[50281, 50281, 50282,  ..., 50283, 50283, 50283],\n",
      "        [50281, 50281, 50282,  ..., 50283, 50283, 50283],\n",
      "        [50281, 50281, 50282,  ..., 50283, 50283, 50283],\n",
      "        [50281, 50281, 50282,  ...,   301, 50284, 50282]])\n",
      "tensor([[ -100,    17,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,    17,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,    17,  -100,  ...,  -100,  -100,  -100],\n",
      "        [ -100,    17,  -100,  ...,  -100, 49651,  -100]])\n"
     ]
    }
   ],
   "source": [
    "class CustomDataCollatorForLanguageModeling(transformers.DataCollatorForLanguageModeling):\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None):\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        labels = inputs.clone()\n",
    "        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        if special_tokens_mask is None:\n",
    "            special_tokens_mask = [\n",
    "                tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
    "            ]\n",
    "            special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "            # block out first three tokens for joint CLS training\n",
    "            special_tokens_mask[:, :3] = True\n",
    "        else:\n",
    "            special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "        probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "        # except second token of each sequence\n",
    "        labels[:, 1] = inputs[:, 1].clone()\n",
    "\n",
    "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
    "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
    "        inputs[indices_replaced] = tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "        # 10% of the time, we replace masked input tokens with random word\n",
    "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
    "        random_words = torch.randint(len(tokenizer), labels.shape, dtype=torch.long)\n",
    "        inputs[indices_random] = random_words[indices_random]\n",
    "\n",
    "        inputs[:, 1] = tokenizer.cls_token_id\n",
    "        print(tokenizer.cls_token_id)\n",
    "\n",
    "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "        return inputs, labels\n",
    "\n",
    "collate_fn = CustomDataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(ds_train, collate_fn=collate_fn, batch_size=4)\n",
    "\n",
    "for example in dataloader:\n",
    "    print(example['input_ids'])\n",
    "    print(example['labels'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS][MASK][SEP]<[MASK]begin'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([50281, 50284, 50282,    29, 50284,  2043])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000000000000019884624838656"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50281"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['chosen', 'rejected']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_columns = [col for col in ds['train'].column_names]\n",
    "rm_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_skywork(ex):\n",
    "    import random\n",
    "    coin = random.randint(0, 1)\n",
    "    if coin == 0:\n",
    "        pairs = [\"0[SEP]\" + chosen + \"[SEP]\" + rejected for chosen, rejected in zip(ex['chosen'], ex['rejected'])]\n",
    "    if coin == 1:\n",
    "        pairs = [\"1[SEP]\" + rejected + \"[SEP]\" + chosen for chosen, rejected in zip(ex['chosen'], ex['rejected'])]\n",
    "\n",
    "    return {\"text\": pairs}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mapped_train = ds['train'].map(preprocess_skywork, batched=True, remove_columns=rm_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_mapped_test = ds['test'].map(preprocess_skywork, batched=True, remove_columns=rm_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 69314\n",
       "})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_mapped_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['train'] = ds_mapped_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds['test'] = ds_mapped_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 420/420 [00:00<00:00, 4.51kB/s]\n",
      "Downloading data: 100%|██████████| 197M/197M [00:04<00:00, 39.7MB/s] \n",
      "Downloading data: 100%|██████████| 22.1M/22.1M [00:00<00:00, 38.0MB/s]\n",
      "Generating train split: 100%|██████████| 69314/69314 [00:01<00:00, 39329.12 examples/s]\n",
      "Generating test split: 100%|██████████| 7702/7702 [00:00<00:00, 35988.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "ds_new = datasets.load_dataset(\"sarahpann/mlm_cls_skywork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 70/70 [00:02<00:00, 33.54ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.53s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 8/8 [00:00<00:00, 32.02ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sarahpann/mlm_cls_skywork/commit/fae176dc52fff7ef259d17559dd2de72a1431111', commit_message='Upload dataset', commit_description='', oid='fae176dc52fff7ef259d17559dd2de72a1431111', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"sarahpann/mlm_cls_skywork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 6/6 [00:00<00:00, 67.65ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/sarahpann/mlm_cls_rewardbench/commit/f2e4725067c580a803a746561d92881fb1e20333', commit_message='Upload dataset', commit_description='', oid='f2e4725067c580a803a746561d92881fb1e20333', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.push_to_hub(\"sarahpann/mlm_cls_rewardbench\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
