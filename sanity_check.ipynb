{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import Optional, cast, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "from src.flex_bert import *\n",
    "from src.evals.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        sliding_window_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        indices: Optional[torch.Tensor] = None,\n",
    "        cu_seqlens: Optional[torch.Tensor] = None,\n",
    "        max_seqlen: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "        seq_len: Optional[int] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        **kwargs,\n",
    "    ) -> Union[Tuple[torch.Tensor], MaskedLMOutput]:\n",
    "        \n",
    "\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        self._maybe_set_compile()\n",
    "\n",
    "        label_copy = labels.clone()\n",
    "        label_copy[:, 2:] = -100\n",
    "\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            if indices is None and cu_seqlens is None and max_seqlen is None:\n",
    "                batch_size, seq_len = input_ids.shape[:2]\n",
    "                if attention_mask is None:\n",
    "                    attention_mask = torch.ones((batch_size, seq_len), device=input_ids.device, dtype=torch.bool)\n",
    "                with torch.no_grad():\n",
    "                    input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels = _unpad_modernbert_input(\n",
    "                        inputs=input_ids, attention_mask=attention_mask, position_ids=position_ids, labels=labels\n",
    "                    )\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            sliding_window_mask=sliding_window_mask,\n",
    "            position_ids=position_ids,\n",
    "            indices=indices,\n",
    "            cu_seqlens=cu_seqlens,\n",
    "            max_seqlen=max_seqlen,\n",
    "            batch_size=batch_size,\n",
    "            seq_len=seq_len,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        last_hidden_state = outputs[0]\n",
    "\n",
    "        if self.sparse_prediction and labels is not None:\n",
    "            # flatten labels and output first\n",
    "            labels = labels.view(-1)\n",
    "            last_hidden_state = last_hidden_state.view(labels.shape[0], -1)\n",
    "\n",
    "            # then filter out the non-masked tokens\n",
    "            mask_tokens = labels != self.sparse_pred_ignore_index\n",
    "            last_hidden_state = last_hidden_state[mask_tokens]\n",
    "            labels = labels[mask_tokens]\n",
    "\n",
    "        logits = (\n",
    "            self.compiled_head(last_hidden_state)\n",
    "            if self.config.reference_compile\n",
    "            else self.decoder(self.head(last_hidden_state))\n",
    "        )\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss = self.loss_function(logits, labels, vocab_size=self.config.vocab_size)\n",
    "\n",
    "        if self.config._attn_implementation == \"flash_attention_2\":\n",
    "            with torch.no_grad():\n",
    "                logits = _pad_modernbert_output(inputs=logits, indices=indices, batch=batch_size, seqlen=seq_len)\n",
    "        if not return_dict:\n",
    "            output = (logits,)\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return MaskedLMOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/yamls/main/flex-bert-base-sarah.yaml\") as f:\n",
    "    yaml_config = om.load(f)\n",
    "\n",
    "cfg = cast(DictConfig, yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertForMaskedLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "model = create_modern_bert_mlm(\n",
    "    pretrained_checkpoint=cfg.model.pretrained_checkpoint,\n",
    "    model_config=cfg.model.model_config,\n",
    "    tokenizer_name=cfg.tokenizer_name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method HuggingFaceModel.forward of EfficientHuggingFaceModel(\n",
       "  (model): MLMxCLSHuggingFaceModel(\n",
       "    (model): ModernBertForMaskedLM(\n",
       "      (model): ModernBertModel(\n",
       "        (embeddings): ModernBertEmbeddings(\n",
       "          (tok_embeddings): Embedding(50368, 1024, padding_idx=50283)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): ModernBertEncoderLayer(\n",
       "            (attn_norm): Identity()\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1-2): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (3): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4-5): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (6): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (7-8): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (9): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (10-11): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (12): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (13-14): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (15): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (16-17): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (18): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (19-20): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (21): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (22-23): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (24): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (25-26): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (27): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (head): ModernBertPredictionHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=50368, bias=True)\n",
       "    )\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientHuggingFaceModel(\n",
       "  (model): MLMxCLSHuggingFaceModel(\n",
       "    (model): ModernBertForMaskedLM(\n",
       "      (model): ModernBertModel(\n",
       "        (embeddings): ModernBertEmbeddings(\n",
       "          (tok_embeddings): Embedding(50368, 1024, padding_idx=50283)\n",
       "          (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layers): ModuleList(\n",
       "          (0): ModernBertEncoderLayer(\n",
       "            (attn_norm): Identity()\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (1-2): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (3): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (4-5): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (6): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (7-8): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (9): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (10-11): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (12): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (13-14): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (15): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (16-17): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (18): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (19-20): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (21): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (22-23): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (24): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (25-26): 2 x ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "          (27): ModernBertEncoderLayer(\n",
       "            (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): ModernBertAttention(\n",
       "              (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "              (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "              (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "              (out_drop): Identity()\n",
       "            )\n",
       "            (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): ModernBertMLP(\n",
       "              (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "              (act): GELUActivation()\n",
       "              (drop): Dropout(p=0.0, inplace=False)\n",
       "              (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (head): ModernBertPredictionHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (act): GELUActivation()\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=1024, out_features=50368, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/temp/ipykernel_1235980/594272676.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/latest-rank0.pt\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bclavie/olmo_bert_template\")\n",
    "state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/latest-rank0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_prefix_in_state_dict_if_present(\n",
    "    state_dict, prefix\n",
    "):\n",
    "    r\"\"\"Strip the prefix in state_dict in place, if any.\n",
    "\n",
    "    ..note::\n",
    "        Given a `state_dict` from a DP/DDP model, a local model can load it by applying\n",
    "        `consume_prefix_in_state_dict_if_present(state_dict, \"module.\")` before calling\n",
    "        :meth:`torch.nn.Module.load_state_dict`.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): a state-dict to be loaded to the model.\n",
    "        prefix (str): prefix.\n",
    "    \"\"\"\n",
    "    keys = sorted(state_dict.keys())\n",
    "    for key in keys:\n",
    "        if key.startswith(prefix):\n",
    "            newkey = key[len(prefix) :]\n",
    "            state_dict[newkey] = state_dict.pop(key)\n",
    "\n",
    "    # also strip the prefix in metadata if any.\n",
    "    if \"_metadata\" in state_dict:\n",
    "        metadata = state_dict[\"_metadata\"]\n",
    "        for key in list(metadata.keys()):\n",
    "            # for the metadata dict, the key can be:\n",
    "            # '': for the DDP module, which we want to remove.\n",
    "            # 'module': for the actual model.\n",
    "            # 'module.xx.xx': for the rest.\n",
    "\n",
    "            if len(key) == 0:\n",
    "                continue\n",
    "            newkey = key[len(prefix) :]\n",
    "            metadata[newkey] = metadata.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = state_dict['state']['model']\n",
    "consume_prefix_in_state_dict_if_present(state_dict, \"model.\")\n",
    "consume_prefix_in_state_dict_if_present(state_dict, \"bert.\")\n",
    "torch.save(state_dict, \"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/correct_names.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are attempting to use Flash Attention 2.0 without specifying a torch dtype. This might lead to unexpected behaviour\n",
      "You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "Flash Attention 2.0 only supports torch.float16 and torch.bfloat16 dtypes, but the current dype in ModernBertForMaskedLM is torch.float32. You should run training or inference using Automatic Mixed-Precision via the `with torch.autocast(device_type='torch_device'):` decorator, or load the model with the `torch_dtype` argument. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16)`\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForMaskedLM.from_pretrained(\"answerdotai/ModernBERT-large\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.model.model_config.sliding_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModernBertForMaskedLM(\n",
       "  (model): ModernBertModel(\n",
       "    (embeddings): ModernBertEmbeddings(\n",
       "      (tok_embeddings): Embedding(50368, 1024, padding_idx=50283)\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): ModernBertEncoderLayer(\n",
       "        (attn_norm): Identity()\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (1-2): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (3): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (4-5): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (6): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (7-8): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (9): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (10-11): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (12): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (13-14): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (15): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (16-17): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (18): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (19-20): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (21): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (22-23): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (24): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (25-26): 2 x ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=10000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "      (27): ModernBertEncoderLayer(\n",
       "        (attn_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ModernBertAttention(\n",
       "          (Wqkv): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "          (rotary_emb): ModernBertUnpaddedRotaryEmbedding(dim=64, base=160000.0, scale_base=None)\n",
       "          (Wo): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "          (out_drop): Identity()\n",
       "        )\n",
       "        (mlp_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModernBertMLP(\n",
       "          (Wi): Linear(in_features=1024, out_features=5248, bias=False)\n",
       "          (act): GELUActivation()\n",
       "          (drop): Dropout(p=0.0, inplace=False)\n",
       "          (Wo): Linear(in_features=2624, out_features=1024, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (head): ModernBertPredictionHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "    (act): GELUActivation()\n",
       "    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): Linear(in_features=1024, out_features=50368, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"sarahpann/mlm_cls_rewardbench\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"0[SEP]What are different drawers I should have for clothes?\\nThe types of drawers you should have for clothes depend on your personal wardrobe and the space you have available. However, here's a general guide for categorizing different types of clothing into drawers:\\n\\n1. **Undergarments Drawer**: This drawer is for your underwear, bras, socks, and hosiery. Some people prefer to separate these further, with a drawer specifically for bras and another for underwear and socks.\\n\\n2. **T-shirts and Casual Tops Drawer**: A drawer for casual wear such as t-shirts, tank tops, and other everyday shirts can help keep your casual wear organized and easily accessible.\\n\\n3. **Pajamas and Lounge Wear Drawer**: This drawer is for your pajamas, nightgowns, and lounge wear, including comfy shorts and sweatpants.\\n\\n4. **Activewear and Gym Clothes Drawer**: If you work out regularly, it's helpful to have a drawer dedicated to gym clothes, sports bras, workout t-shirts, leggings, and shorts.\\n\\n5. **Jeans and Casual Pants Drawer**: Designate a drawer for your jeans, casual pants, and other heavier items that can be folded without concern for wrinkles.\\n\\n6. **Sweaters and Knits Drawer**: Sweaters, cardigans, and knitted tops should have their own drawer. Its often recommended to fold knitted garments to prevent them from stretching out on hangers.\\n\\n7. **Dress Shirts and Blouses Drawer**: For more formal or wrinkle-prone tops, you may want a separate drawer. Some may prefer to hang these items instead if space allows.\\n\\n8. **Shorts and Skirts Drawer**: If you have a collection of shorts and skirts, these can go together in a drawer. You might separate them by length or occasion (casual vs. dressy).\\n\\n9. **Accessories Drawer**: For belts, scarves, and other small accessories, a drawer with dividers can be very useful to keep these items from becoming tangled or lost.\\n\\n10. **Seasonal Drawer**: Depending on the climate you live in, you might have a drawer for seasonal items that you only wear at certain times of the year, such as swimwear or heavy thermal clothing.\\n\\nRemember that these are just suggestions, and the best setup for you will depend on what types of clothing you own and how much of each type you have. If you have a smaller wardrobe, you may combine categories into a single drawer. Conversely, if you have a large collection of a particular type of clothing, it may require multiple drawers. Drawer dividers or organizers can be very helpful in keeping items separate and easy to find within each drawer.[SEP]What are different drawers I should have for clothes?\\nDifferent drawers you should have for clothes include a dresser, a jewelry box, a sock drawer, a shoe drawer, and a delicates drawer.\"}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-large\")\n",
    "def mask_tokens(ex):\n",
    "    new_text = \"[CLS]\" + ex['text'][1:]\n",
    "    return {\"text\": new_text, \"labels\": ex['text']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"[CLS][SEP]What are different drawers I should have for clothes?\\nThe types of drawers you should have for clothes depend on your personal wardrobe and the space you have available. However, here's a general guide for categorizing different types of clothing into drawers:\\n\\n1. **Undergarments Drawer**: This drawer is for your underwear, bras, socks, and hosiery. Some people prefer to separate these further, with a drawer specifically for bras and another for underwear and socks.\\n\\n2. **T-shirts and Casual Tops Drawer**: A drawer for casual wear such as t-shirts, tank tops, and other everyday shirts can help keep your casual wear organized and easily accessible.\\n\\n3. **Pajamas and Lounge Wear Drawer**: This drawer is for your pajamas, nightgowns, and lounge wear, including comfy shorts and sweatpants.\\n\\n4. **Activewear and Gym Clothes Drawer**: If you work out regularly, it's helpful to have a drawer dedicated to gym clothes, sports bras, workout t-shirts, leggings, and shorts.\\n\\n5. **Jeans and Casual Pants Drawer**: Designate a drawer for your jeans, casual pants, and other heavier items that can be folded without concern for wrinkles.\\n\\n6. **Sweaters and Knits Drawer**: Sweaters, cardigans, and knitted tops should have their own drawer. Its often recommended to fold knitted garments to prevent them from stretching out on hangers.\\n\\n7. **Dress Shirts and Blouses Drawer**: For more formal or wrinkle-prone tops, you may want a separate drawer. Some may prefer to hang these items instead if space allows.\\n\\n8. **Shorts and Skirts Drawer**: If you have a collection of shorts and skirts, these can go together in a drawer. You might separate them by length or occasion (casual vs. dressy).\\n\\n9. **Accessories Drawer**: For belts, scarves, and other small accessories, a drawer with dividers can be very useful to keep these items from becoming tangled or lost.\\n\\n10. **Seasonal Drawer**: Depending on the climate you live in, you might have a drawer for seasonal items that you only wear at certain times of the year, such as swimwear or heavy thermal clothing.\\n\\nRemember that these are just suggestions, and the best setup for you will depend on what types of clothing you own and how much of each type you have. If you have a smaller wardrobe, you may combine categories into a single drawer. Conversely, if you have a large collection of a particular type of clothing, it may require multiple drawers. Drawer dividers or organizers can be very helpful in keeping items separate and easy to find within each drawer.[SEP]What are different drawers I should have for clothes?\\nDifferent drawers you should have for clothes include a dresser, a jewelry box, a sock drawer, a shoe drawer, and a delicates drawer.\", 'labels': \"0[SEP]What are different drawers I should have for clothes?\\nThe types of drawers you should have for clothes depend on your personal wardrobe and the space you have available. However, here's a general guide for categorizing different types of clothing into drawers:\\n\\n1. **Undergarments Drawer**: This drawer is for your underwear, bras, socks, and hosiery. Some people prefer to separate these further, with a drawer specifically for bras and another for underwear and socks.\\n\\n2. **T-shirts and Casual Tops Drawer**: A drawer for casual wear such as t-shirts, tank tops, and other everyday shirts can help keep your casual wear organized and easily accessible.\\n\\n3. **Pajamas and Lounge Wear Drawer**: This drawer is for your pajamas, nightgowns, and lounge wear, including comfy shorts and sweatpants.\\n\\n4. **Activewear and Gym Clothes Drawer**: If you work out regularly, it's helpful to have a drawer dedicated to gym clothes, sports bras, workout t-shirts, leggings, and shorts.\\n\\n5. **Jeans and Casual Pants Drawer**: Designate a drawer for your jeans, casual pants, and other heavier items that can be folded without concern for wrinkles.\\n\\n6. **Sweaters and Knits Drawer**: Sweaters, cardigans, and knitted tops should have their own drawer. Its often recommended to fold knitted garments to prevent them from stretching out on hangers.\\n\\n7. **Dress Shirts and Blouses Drawer**: For more formal or wrinkle-prone tops, you may want a separate drawer. Some may prefer to hang these items instead if space allows.\\n\\n8. **Shorts and Skirts Drawer**: If you have a collection of shorts and skirts, these can go together in a drawer. You might separate them by length or occasion (casual vs. dressy).\\n\\n9. **Accessories Drawer**: For belts, scarves, and other small accessories, a drawer with dividers can be very useful to keep these items from becoming tangled or lost.\\n\\n10. **Seasonal Drawer**: Depending on the climate you live in, you might have a drawer for seasonal items that you only wear at certain times of the year, such as swimwear or heavy thermal clothing.\\n\\nRemember that these are just suggestions, and the best setup for you will depend on what types of clothing you own and how much of each type you have. If you have a smaller wardrobe, you may combine categories into a single drawer. Conversely, if you have a large collection of a particular type of clothing, it may require multiple drawers. Drawer dividers or organizers can be very helpful in keeping items separate and easy to find within each drawer.[SEP]What are different drawers I should have for clothes?\\nDifferent drawers you should have for clothes include a dresser, a jewelry box, a sock drawer, a shoe drawer, and a delicates drawer.\"}\n",
      "{'input_ids': tensor([[50281, 50281, 50282,  1276,   403,  1027,  3812,   398,   309,   943,\n",
      "           452,   323, 10015,    32,   187,   510,  3510,   273,  3812,   398,\n",
      "           368,   943,   452,   323, 10015,  3469,   327,   634,  3367, 43530,\n",
      "           285,   253,  2317,   368,   452,  2130,    15,  1723,    13,  1060,\n",
      "           434,   247,  2087,  7102,   323, 13213,  3006,  1027,  3510,   273,\n",
      "         14234,   715,  3812,   398,    27,   187,   187,    18,    15,  1401,\n",
      "         11752,  5209,   942, 24440,   254, 25593,   831, 31579,   310,   323,\n",
      "           634, 42500,    13, 41797,    13, 33957,    13,   285,   288,   375,\n",
      "          1321,    90,    15,  3808,   952,  4510,   281,  4858,   841,  2007,\n",
      "            13,   342,   247, 31579,  5742,   323, 41797,   285,  1529,   323,\n",
      "         42500,   285, 33957,    15,   187,   187,    19,    15,  1401,    53,\n",
      "            14, 39529,   285, 48357,   308,  2695, 24440,   254, 25593,   329,\n",
      "         31579,   323, 15120,  8251,   824,   347,   246,    14, 39529,    13,\n",
      "         11100, 27164,    13,   285,   643, 15363, 32622,   476,  1361,  1978,\n",
      "           634, 15120,  8251, 10932,   285,  4354, 12482,    15,   187,   187,\n",
      "            20,    15,  1401,    49,  1432, 33122,   285,   418, 27351,   411,\n",
      "           613, 24440,   254, 25593,   831, 31579,   310,   323,   634,  1349,\n",
      "         25402,   284,    13,  2360,    72,   628,    84,    13,   285, 36591,\n",
      "          8251,    13,  1690,   389, 31296, 32353,   285, 17052,    81,  1103,\n",
      "            15,   187,   187,    21,    15,  1401, 16754, 21417,   285, 48634,\n",
      "          1639,  4521, 24440,   254, 25593,  1310,   368,   789,   562, 11719,\n",
      "            13,   352,   434,  9371,   281,   452,   247, 31579,  9940,   281,\n",
      "         17409, 10015,    13,  9001, 41797,    13, 30755,   246,    14, 39529,\n",
      "            13,   458,  1266,   723,    13,   285, 32353,    15,   187,   187,\n",
      "            22,    15,  1401, 20723,   507,   285, 48357,   367,  1103, 24440,\n",
      "           254, 25593, 11405,   366,   247, 31579,   323,   634, 27929,    13,\n",
      "         15120, 19110,    13,   285,   643, 27953,  4957,   326,   476,   320,\n",
      "         20618,  1293,  4468,   323,  1488, 43572,    15,   187,   187,    23,\n",
      "            15,  1401,    52,   664, 20264,   285, 10381,   953, 24440,   254,\n",
      "         25593, 38583, 20264,    13,  3120,   304,   507,    13,   285,   694,\n",
      "          2166, 27164,   943,   452,   616,  1211, 31579,    15,   733,   457,\n",
      "            84,  2223,  8521,   281,  7975,   694,  2166, 42179,   281,  3657,\n",
      "           731,   432, 23148,   562,   327, 10913,   398,    15,   187,   187,\n",
      "            24,    15,  1401,    37,   560,  1608, 14068,   285,  2071, 15011,\n",
      "         24440,   254, 25593,  1198,   625,  7473,   390,  1488, 22064,    14,\n",
      "          1087,   531, 27164,    13,   368,   778,   971,   247,  4858, 31579,\n",
      "            15,  3808,   778,  4510,   281, 10913,   841,  4957,  3185,   604,\n",
      "          2317,  4483,    15,   187,   187,    25,    15,  1401,  2809,  8707,\n",
      "           285,  8229, 14068, 24440,   254, 25593,  1310,   368,   452,   247,\n",
      "          4849,   273, 32353,   285, 48393,    13,   841,   476,   564,  2366,\n",
      "           275,   247, 31579,    15,  1422,  1537,  4858,   731,   407,  2978,\n",
      "           390,  8120,   313, 16559,   780,  4632,    15,  7619,    90,   481,\n",
      "           187,   187,    26,    15,  1401, 11501,  2370, 24440,   254, 25593,\n",
      "          1198, 39978,    13, 12689,  1634,    13,   285,   643,  1355, 28234,\n",
      "            13,   247, 31579,   342, 15399,   398,   476,   320,  1077,  4217,\n",
      "           281,  1978,   841,  4957,   432,  7552, 49515,   390,  3663,    15,\n",
      "           187,   187,   740,    15,  1401, 38547,   267, 24440,   254, 25593,\n",
      "         27742,   327,   253,  7952,   368,  3153,   275,    13,   368,  1537,\n",
      "           452,   247, 31579,   323, 22952,  4957,   326,   368,   760,  8251,\n",
      "           387,  2176,  2069,   273,   253,   807,    13,   824,   347, 10831,\n",
      "         21417,   390,  5536,  8609, 14234,    15,   187,   187, 21914,   326,\n",
      "           841,   403,   816, 13991,    13,   285,   253,  1682,  9978,   323,\n",
      "           368,   588,  3469,   327,   752,  3510,   273, 14234,   368,  1211,\n",
      "           285,   849,  1199,   273,  1016,  1511,   368,   452,    15,  1310,\n",
      "           368,   452,   247,  4577, 43530,    13,   368,   778, 13398,  9050,\n",
      "           715,   247,  2014, 31579,    15, 24646,    13,   604,   368,   452,\n",
      "           247,  1781,  4849,   273,   247,  1798,  1511,   273, 14234,    13,\n",
      "           352,   778,  2430,  2709,  3812,   398,    15, 24440,   254, 15399,\n",
      "           398,   390, 37630,   476,   320,  1077,  9371,   275,  7562,  4957,\n",
      "          4858,   285,  3477,   281,  1089,  1561,  1016, 31579,    15, 50282,\n",
      "          1276,   403,  1027,  3812,   398,   309,   943,   452,   323, 10015,\n",
      "            32,   187, 21956,  3812,   398,   368,   943,   452,   323, 10015,\n",
      "          2486,   247,  7619,   254,    13,   247, 28226,  3817,    13,   247,\n",
      "         32584, 31579,    13,   247, 22756, 31579,    13,   285,   247,  1448,\n",
      "         31290, 31579,    15, 50282]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m tokenized[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m new_tokenized \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_ids, \n\u001b[1;32m      8\u001b[0m                  \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m: attention_mask,\n\u001b[1;32m      9\u001b[0m                  }\n\u001b[0;32m---> 10\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/composer/models/huggingface.py:488\u001b[0m, in \u001b[0;36mHuggingFaceModel.forward\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(batch, Mapping):\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;66;03m# Further input validation is left to the huggingface forward call\u001b[39;00m\n\u001b[1;32m    487\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward_args}\n\u001b[0;32m--> 488\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore (thirdparty)\u001b[39;00m\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    490\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    491\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnexpected batch type. Expected a dictionary with keys corresponding to the inputs to the forward function of the Huggingface model\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    492\u001b[0m     )\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:1059\u001b[0m, in \u001b[0;36mModernBertForMaskedLM.forward\u001b[0;34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, labels, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m   1055\u001b[0m             input_ids, indices, cu_seqlens, max_seqlen, position_ids, labels \u001b[38;5;241m=\u001b[39m _unpad_modernbert_input(\n\u001b[1;32m   1056\u001b[0m                 inputs\u001b[38;5;241m=\u001b[39minput_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask, position_ids\u001b[38;5;241m=\u001b[39mposition_ids, labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m   1057\u001b[0m             )\n\u001b[0;32m-> 1059\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1062\u001b[0m \u001b[43m    \u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1063\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1064\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1065\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1066\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1067\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1068\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseq_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1070\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1071\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1072\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m last_hidden_state \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse_prediction \u001b[38;5;129;01mand\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# flatten labels and output first\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:913\u001b[0m, in \u001b[0;36mModernBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, sliding_window_mask, position_ids, indices, cu_seqlens, max_seqlen, batch_size, seq_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    902\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    903\u001b[0m         encoder_layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    904\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         output_attentions,\n\u001b[1;32m    911\u001b[0m     )\n\u001b[1;32m    912\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 913\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mencoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    920\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    921\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    922\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:529\u001b[0m, in \u001b[0;36mModernBertEncoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, sliding_window_mask, position_ids, cu_seqlens, max_seqlen, output_attentions)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    520\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    521\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    527\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    528\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 529\u001b[0m     attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43msliding_window_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msliding_window_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    533\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    534\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    535\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    536\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    537\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    539\u001b[0m     mlp_output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    540\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompiled_mlp(hidden_states)\n\u001b[1;32m    541\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mreference_compile\n\u001b[1;32m    542\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_norm(hidden_states))\n\u001b[1;32m    543\u001b[0m     )\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:487\u001b[0m, in \u001b[0;36mModernBertAttention.forward\u001b[0;34m(self, hidden_states, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mview(bs, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m--> 487\u001b[0m attn_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mMODERNBERT_ATTENTION_FUNCTION\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_implementation\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_attention\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlocal_attention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_head_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    497\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m attn_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    498\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_drop(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mWo(hidden_states))\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:349\u001b[0m, in \u001b[0;36mflash_attention_forward\u001b[0;34m(module, qkv, rotary_emb, cu_seqlens, max_seqlen, local_attention, bs, dim, target_dtype, **_kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflash_attention_forward\u001b[39m(\n\u001b[1;32m    337\u001b[0m     module: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModernBertAttention\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    338\u001b[0m     qkv: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    347\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    348\u001b[0m     \u001b[38;5;66;03m# (total_seqlen, 3, nheads, headdim)\u001b[39;00m\n\u001b[0;32m--> 349\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     convert_dtype \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39mfloat16, torch\u001b[38;5;241m.\u001b[39mbfloat16)\n\u001b[1;32m    352\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_dtype:\n\u001b[1;32m    353\u001b[0m         \u001b[38;5;66;03m# FA2 implementation only supports fp16 and bf16. If FA2 is supported,\u001b[39;00m\n\u001b[1;32m    354\u001b[0m         \u001b[38;5;66;03m# bfloat16 must be supported as of FA2 2.5.7. (Turing GPUs not supported)\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:178\u001b[0m, in \u001b[0;36mModernBertUnpaddedRotaryEmbedding.forward\u001b[0;34m(self, qkv, cu_seqlens, max_seqlen)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m max_seqlen \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_cos_sin_cache(max_seqlen, device\u001b[38;5;241m=\u001b[39mqkv\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mqkv\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m--> 178\u001b[0m qkv \u001b[38;5;241m=\u001b[39m \u001b[43mapply_rotary_unpadded\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cos_cached\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sin_cached\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m qkv\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:136\u001b[0m, in \u001b[0;36mapply_rotary_unpadded\u001b[0;34m(qkv, cos, sin, cu_seqlens, max_seqlen)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_rotary_unpadded\u001b[39m(\n\u001b[1;32m    114\u001b[0m     qkv,\n\u001b[1;32m    115\u001b[0m     cos,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    118\u001b[0m     max_seqlen: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    119\u001b[0m ):\n\u001b[1;32m    120\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;03m        qkv: (total_nnz, 3, nheads, headdim) - input tensor for packed QKV.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03m    Apply rotary embedding to the first rotary_dim of x.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mApplyRotaryEmbUnpad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqkv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/torch/autograd/function.py:574\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    573\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 574\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    576\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[1;32m    577\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    579\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    580\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    581\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    582\u001b[0m     )\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/models/modernbert/modeling_modernbert.py:75\u001b[0m, in \u001b[0;36mApplyRotaryEmbUnpad.forward\u001b[0;34m(ctx, qkv, cos, sin, cu_seqlens, max_seqlen)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# We need qkv to be contiguous so that when we reshape to combine (3, nheads) dimensions,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# we get the same tensor\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# qk = rearrange(qkv[:, :2], \"b_s t h d -> b_s (t h) d\")\u001b[39;00m\n\u001b[1;32m     74\u001b[0m qk \u001b[38;5;241m=\u001b[39m qkv[:, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mview(total_nnz, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, headdim)\n\u001b[0;32m---> 75\u001b[0m \u001b[43mapply_rotary\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[43m    \u001b[49m\u001b[43mqk\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m    \u001b[49m\u001b[43mseqlen_offsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_seqlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_seqlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterleaved\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(cos, sin, cu_seqlens)\n\u001b[1;32m     87\u001b[0m ctx\u001b[38;5;241m.\u001b[39mmax_seqlen \u001b[38;5;241m=\u001b[39m max_seqlen\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/rotary.py:202\u001b[0m, in \u001b[0;36mapply_rotary\u001b[0;34m(x, cos, sin, seqlen_offsets, cu_seqlens, max_seqlen, interleaved, inplace, conjugate)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Need this, otherwise Triton tries to launch from cuda:0 and we get\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;66;03m# ValueError: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(x\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mindex):\n\u001b[0;32m--> 202\u001b[0m     \u001b[43mrotary_kernel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgrid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# data ptrs\u001b[39;49;00m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43msin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcu_seqlens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen_offsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# shapes\u001b[39;49;00m\n\u001b[1;32m    210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrotary_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseqlen_ro\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_varlen\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch_strides if not varlen else 0\u001b[39;49;00m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# seqlen_stride or total_seqlen_stride\u001b[39;49;00m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# nheads_stride\u001b[39;49;00m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# headdim_stride\u001b[39;49;00m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mis_varlen\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# batch_strides if not varlen else 0\u001b[39;49;00m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# seqlen stride or total_seqlen_stride\u001b[39;49;00m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# nheads stride\u001b[39;49;00m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# headdim stride\u001b[39;49;00m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBLOCK_K\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mseqlen_offsets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_varlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterleaved\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconjugate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mBLOCK_M\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/triton/runtime/jit.py:345\u001b[0m, in \u001b[0;36mKernelInterface.__getitem__.<locals>.<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, grid) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[1;32m    340\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;124;03m    A JIT function is launched with: fn[grid](*args, **kwargs).\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m    Hence JITFunction.__getitem__ returns a callable proxy that\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m    memorizes the grid.\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwarmup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/triton/runtime/jit.py:691\u001b[0m, in \u001b[0;36mJITFunction.run\u001b[0;34m(self, grid, warmup, *args, **kwargs)\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[38;5;66;03m# launch kernel\u001b[39;00m\n\u001b[1;32m    690\u001b[0m     launch_metadata \u001b[38;5;241m=\u001b[39m kernel\u001b[38;5;241m.\u001b[39mlaunch_metadata(grid, stream, \u001b[38;5;241m*\u001b[39mnon_constexpr_vals)\n\u001b[0;32m--> 691\u001b[0m     \u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrid_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrid_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpacked_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlaunch_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiledKernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_enter_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompiledKernel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch_exit_hook\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnon_constexpr_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m kernel\n",
      "File \u001b[0;32m/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/triton/backends/nvidia/driver.py:365\u001b[0m, in \u001b[0;36mCudaLauncher.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 365\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlaunch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Pointer argument (at 0) cannot be accessed from Triton (cpu tensor?)"
     ]
    }
   ],
   "source": [
    "masked_in = mask_tokens(ds[8])\n",
    "print(masked_in)\n",
    "tokenized = tokenizer(masked_in['text'], return_tensors='pt')\n",
    "print(tokenized)\n",
    "input_ids = tokenized['input_ids'].to(device='cuda')\n",
    "attention_mask = tokenized['attention_mask'].to(device='cuda')\n",
    "new_tokenized = {\"input_ids\": input_ids, \n",
    "                 \"attention_mask\": attention_mask,\n",
    "                 }\n",
    "output = model(tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991, 13537,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991, 13537,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991, 13537,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "        13537,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991, 13537,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991, 13537,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991, 13537,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "        13537,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991, 13537,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991, 13537,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991, 13537,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991, 13537,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991, 13537,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991, 13537,   991,   991, 13537,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991,   991,   991,   991,   991,   991,   991,\n",
       "          991,   991,   991,   991], device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.argmax(output.logits, dim=-1)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ax'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
