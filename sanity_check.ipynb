{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:958: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/home/public/span/miniconda3/envs/bert24/lib/python3.11/site-packages/flash_attn/ops/triton/layer_norm.py:1017: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "from typing import Optional, cast, Dict, Any\n",
    "\n",
    "import torch\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "import datasets\n",
    "\n",
    "from omegaconf import DictConfig\n",
    "from omegaconf import OmegaConf as om\n",
    "\n",
    "from src.flex_bert import *\n",
    "from src.evals.data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/yamls/main/flex-bert-base-sarah.yaml\") as f:\n",
    "    yaml_config = om.load(f)\n",
    "\n",
    "cfg = cast(DictConfig, yaml_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/temp/ipykernel_885358/594272676.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/latest-rank0.pt\")\n"
     ]
    }
   ],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bclavie/olmo_bert_template\")\n",
    "state_dict = torch.load(\"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/latest-rank0.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consume_prefix_in_state_dict_if_present(\n",
    "    state_dict, prefix\n",
    "):\n",
    "    r\"\"\"Strip the prefix in state_dict in place, if any.\n",
    "\n",
    "    ..note::\n",
    "        Given a `state_dict` from a DP/DDP model, a local model can load it by applying\n",
    "        `consume_prefix_in_state_dict_if_present(state_dict, \"module.\")` before calling\n",
    "        :meth:`torch.nn.Module.load_state_dict`.\n",
    "\n",
    "    Args:\n",
    "        state_dict (OrderedDict): a state-dict to be loaded to the model.\n",
    "        prefix (str): prefix.\n",
    "    \"\"\"\n",
    "    keys = sorted(state_dict.keys())\n",
    "    for key in keys:\n",
    "        if key.startswith(prefix):\n",
    "            newkey = key[len(prefix) :]\n",
    "            state_dict[newkey] = state_dict.pop(key)\n",
    "\n",
    "    # also strip the prefix in metadata if any.\n",
    "    if \"_metadata\" in state_dict:\n",
    "        metadata = state_dict[\"_metadata\"]\n",
    "        for key in list(metadata.keys()):\n",
    "            # for the metadata dict, the key can be:\n",
    "            # '': for the DDP module, which we want to remove.\n",
    "            # 'module': for the actual model.\n",
    "            # 'module.xx.xx': for the rest.\n",
    "\n",
    "            if len(key) == 0:\n",
    "                continue\n",
    "            newkey = key[len(prefix) :]\n",
    "            metadata[newkey] = metadata.pop(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = state_dict['state']['model']\n",
    "consume_prefix_in_state_dict_if_present(state_dict, \"model.\")\n",
    "torch.save(state_dict, \"/home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/correct_names.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/public/span/MATH_DPO/modern_bert_test/bert24/src/bert_layers/model.py:1063: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(pretrained_checkpoint)\n",
      "Found these missing keys in the checkpoint: head.norm.weight, decoder.weight, decoder.bias\n",
      "Found these unexpected keys in the checkpoint: classifier.bias, classifier.weight\n"
     ]
    }
   ],
   "source": [
    "model = create_flex_bert_mlm(\n",
    "    pretrained_checkpoint=cfg.model.pretrained_checkpoint,\n",
    "    model_config=cfg.model.model_config,\n",
    "    tokenizer_name=cfg.tokenizer_name\n",
    ")\n",
    "model = model.model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = datasets.load_dataset(\"sarahpann/mlm_cls_rewardbench\")['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"0[SEP]What are different drawers I should have for clothes?\\nThe types of drawers you should have for clothes depend on your personal wardrobe and the space you have available. However, here's a general guide for categorizing different types of clothing into drawers:\\n\\n1. **Undergarments Drawer**: This drawer is for your underwear, bras, socks, and hosiery. Some people prefer to separate these further, with a drawer specifically for bras and another for underwear and socks.\\n\\n2. **T-shirts and Casual Tops Drawer**: A drawer for casual wear such as t-shirts, tank tops, and other everyday shirts can help keep your casual wear organized and easily accessible.\\n\\n3. **Pajamas and Lounge Wear Drawer**: This drawer is for your pajamas, nightgowns, and lounge wear, including comfy shorts and sweatpants.\\n\\n4. **Activewear and Gym Clothes Drawer**: If you work out regularly, it's helpful to have a drawer dedicated to gym clothes, sports bras, workout t-shirts, leggings, and shorts.\\n\\n5. **Jeans and Casual Pants Drawer**: Designate a drawer for your jeans, casual pants, and other heavier items that can be folded without concern for wrinkles.\\n\\n6. **Sweaters and Knits Drawer**: Sweaters, cardigans, and knitted tops should have their own drawer. It’s often recommended to fold knitted garments to prevent them from stretching out on hangers.\\n\\n7. **Dress Shirts and Blouses Drawer**: For more formal or wrinkle-prone tops, you may want a separate drawer. Some may prefer to hang these items instead if space allows.\\n\\n8. **Shorts and Skirts Drawer**: If you have a collection of shorts and skirts, these can go together in a drawer. You might separate them by length or occasion (casual vs. dressy).\\n\\n9. **Accessories Drawer**: For belts, scarves, and other small accessories, a drawer with dividers can be very useful to keep these items from becoming tangled or lost.\\n\\n10. **Seasonal Drawer**: Depending on the climate you live in, you might have a drawer for seasonal items that you only wear at certain times of the year, such as swimwear or heavy thermal clothing.\\n\\nRemember that these are just suggestions, and the best setup for you will depend on what types of clothing you own and how much of each type you have. If you have a smaller wardrobe, you may combine categories into a single drawer. Conversely, if you have a large collection of a particular type of clothing, it may require multiple drawers. Drawer dividers or organizers can be very helpful in keeping items separate and easy to find within each drawer.[SEP]What are different drawers I should have for clothes?\\nDifferent drawers you should have for clothes include a dresser, a jewelry box, a sock drawer, a shoe drawer, and a delicates drawer.\"}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_tokens(ex):\n",
    "    new_text = \"[CLS]\" + ex['text'][1:]\n",
    "    return {\"text\": new_text, \"labels\": ex['text']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"[CLS][SEP]What are different drawers I should have for clothes?\\nThe types of drawers you should have for clothes depend on your personal wardrobe and the space you have available. However, here's a general guide for categorizing different types of clothing into drawers:\\n\\n1. **Undergarments Drawer**: This drawer is for your underwear, bras, socks, and hosiery. Some people prefer to separate these further, with a drawer specifically for bras and another for underwear and socks.\\n\\n2. **T-shirts and Casual Tops Drawer**: A drawer for casual wear such as t-shirts, tank tops, and other everyday shirts can help keep your casual wear organized and easily accessible.\\n\\n3. **Pajamas and Lounge Wear Drawer**: This drawer is for your pajamas, nightgowns, and lounge wear, including comfy shorts and sweatpants.\\n\\n4. **Activewear and Gym Clothes Drawer**: If you work out regularly, it's helpful to have a drawer dedicated to gym clothes, sports bras, workout t-shirts, leggings, and shorts.\\n\\n5. **Jeans and Casual Pants Drawer**: Designate a drawer for your jeans, casual pants, and other heavier items that can be folded without concern for wrinkles.\\n\\n6. **Sweaters and Knits Drawer**: Sweaters, cardigans, and knitted tops should have their own drawer. It’s often recommended to fold knitted garments to prevent them from stretching out on hangers.\\n\\n7. **Dress Shirts and Blouses Drawer**: For more formal or wrinkle-prone tops, you may want a separate drawer. Some may prefer to hang these items instead if space allows.\\n\\n8. **Shorts and Skirts Drawer**: If you have a collection of shorts and skirts, these can go together in a drawer. You might separate them by length or occasion (casual vs. dressy).\\n\\n9. **Accessories Drawer**: For belts, scarves, and other small accessories, a drawer with dividers can be very useful to keep these items from becoming tangled or lost.\\n\\n10. **Seasonal Drawer**: Depending on the climate you live in, you might have a drawer for seasonal items that you only wear at certain times of the year, such as swimwear or heavy thermal clothing.\\n\\nRemember that these are just suggestions, and the best setup for you will depend on what types of clothing you own and how much of each type you have. If you have a smaller wardrobe, you may combine categories into a single drawer. Conversely, if you have a large collection of a particular type of clothing, it may require multiple drawers. Drawer dividers or organizers can be very helpful in keeping items separate and easy to find within each drawer.[SEP]What are different drawers I should have for clothes?\\nDifferent drawers you should have for clothes include a dresser, a jewelry box, a sock drawer, a shoe drawer, and a delicates drawer.\", 'labels': \"0[SEP]What are different drawers I should have for clothes?\\nThe types of drawers you should have for clothes depend on your personal wardrobe and the space you have available. However, here's a general guide for categorizing different types of clothing into drawers:\\n\\n1. **Undergarments Drawer**: This drawer is for your underwear, bras, socks, and hosiery. Some people prefer to separate these further, with a drawer specifically for bras and another for underwear and socks.\\n\\n2. **T-shirts and Casual Tops Drawer**: A drawer for casual wear such as t-shirts, tank tops, and other everyday shirts can help keep your casual wear organized and easily accessible.\\n\\n3. **Pajamas and Lounge Wear Drawer**: This drawer is for your pajamas, nightgowns, and lounge wear, including comfy shorts and sweatpants.\\n\\n4. **Activewear and Gym Clothes Drawer**: If you work out regularly, it's helpful to have a drawer dedicated to gym clothes, sports bras, workout t-shirts, leggings, and shorts.\\n\\n5. **Jeans and Casual Pants Drawer**: Designate a drawer for your jeans, casual pants, and other heavier items that can be folded without concern for wrinkles.\\n\\n6. **Sweaters and Knits Drawer**: Sweaters, cardigans, and knitted tops should have their own drawer. It’s often recommended to fold knitted garments to prevent them from stretching out on hangers.\\n\\n7. **Dress Shirts and Blouses Drawer**: For more formal or wrinkle-prone tops, you may want a separate drawer. Some may prefer to hang these items instead if space allows.\\n\\n8. **Shorts and Skirts Drawer**: If you have a collection of shorts and skirts, these can go together in a drawer. You might separate them by length or occasion (casual vs. dressy).\\n\\n9. **Accessories Drawer**: For belts, scarves, and other small accessories, a drawer with dividers can be very useful to keep these items from becoming tangled or lost.\\n\\n10. **Seasonal Drawer**: Depending on the climate you live in, you might have a drawer for seasonal items that you only wear at certain times of the year, such as swimwear or heavy thermal clothing.\\n\\nRemember that these are just suggestions, and the best setup for you will depend on what types of clothing you own and how much of each type you have. If you have a smaller wardrobe, you may combine categories into a single drawer. Conversely, if you have a large collection of a particular type of clothing, it may require multiple drawers. Drawer dividers or organizers can be very helpful in keeping items separate and easy to find within each drawer.[SEP]What are different drawers I should have for clothes?\\nDifferent drawers you should have for clothes include a dresser, a jewelry box, a sock drawer, a shoe drawer, and a delicates drawer.\"}\n",
      "{'input_ids': tensor([[50281, 50281, 50282,  1276,   403,  1027,  3812,   398,   309,   943,\n",
      "           452,   323, 10015,    32,   187,   510,  3510,   273,  3812,   398,\n",
      "           368,   943,   452,   323, 10015,  3469,   327,   634,  3367, 43530,\n",
      "           285,   253,  2317,   368,   452,  2130,    15,  1723,    13,  1060,\n",
      "           434,   247,  2087,  7102,   323, 13213,  3006,  1027,  3510,   273,\n",
      "         14234,   715,  3812,   398,    27,   187,   187,    18,    15,  1401,\n",
      "         11752,  5209,   942, 24440,   254, 25593,   831, 31579,   310,   323,\n",
      "           634, 42500,    13, 41797,    13, 33957,    13,   285,   288,   375,\n",
      "          1321,    90,    15,  3808,   952,  4510,   281,  4858,   841,  2007,\n",
      "            13,   342,   247, 31579,  5742,   323, 41797,   285,  1529,   323,\n",
      "         42500,   285, 33957,    15,   187,   187,    19,    15,  1401,    53,\n",
      "            14, 39529,   285, 48357,   308,  2695, 24440,   254, 25593,   329,\n",
      "         31579,   323, 15120,  8251,   824,   347,   246,    14, 39529,    13,\n",
      "         11100, 27164,    13,   285,   643, 15363, 32622,   476,  1361,  1978,\n",
      "           634, 15120,  8251, 10932,   285,  4354, 12482,    15,   187,   187,\n",
      "            20,    15,  1401,    49,  1432, 33122,   285,   418, 27351,   411,\n",
      "           613, 24440,   254, 25593,   831, 31579,   310,   323,   634,  1349,\n",
      "         25402,   284,    13,  2360,    72,   628,    84,    13,   285, 36591,\n",
      "          8251,    13,  1690,   389, 31296, 32353,   285, 17052,    81,  1103,\n",
      "            15,   187,   187,    21,    15,  1401, 16754, 21417,   285, 48634,\n",
      "          1639,  4521, 24440,   254, 25593,  1310,   368,   789,   562, 11719,\n",
      "            13,   352,   434,  9371,   281,   452,   247, 31579,  9940,   281,\n",
      "         17409, 10015,    13,  9001, 41797,    13, 30755,   246,    14, 39529,\n",
      "            13,   458,  1266,   723,    13,   285, 32353,    15,   187,   187,\n",
      "            22,    15,  1401, 20723,   507,   285, 48357,   367,  1103, 24440,\n",
      "           254, 25593, 11405,   366,   247, 31579,   323,   634, 27929,    13,\n",
      "         15120, 19110,    13,   285,   643, 27953,  4957,   326,   476,   320,\n",
      "         20618,  1293,  4468,   323,  1488, 43572,    15,   187,   187,    23,\n",
      "            15,  1401,    52,   664, 20264,   285, 10381,   953, 24440,   254,\n",
      "         25593, 38583, 20264,    13,  3120,   304,   507,    13,   285,   694,\n",
      "          2166, 27164,   943,   452,   616,  1211, 31579,    15,   733,   457,\n",
      "            84,  2223,  8521,   281,  7975,   694,  2166, 42179,   281,  3657,\n",
      "           731,   432, 23148,   562,   327, 10913,   398,    15,   187,   187,\n",
      "            24,    15,  1401,    37,   560,  1608, 14068,   285,  2071, 15011,\n",
      "         24440,   254, 25593,  1198,   625,  7473,   390,  1488, 22064,    14,\n",
      "          1087,   531, 27164,    13,   368,   778,   971,   247,  4858, 31579,\n",
      "            15,  3808,   778,  4510,   281, 10913,   841,  4957,  3185,   604,\n",
      "          2317,  4483,    15,   187,   187,    25,    15,  1401,  2809,  8707,\n",
      "           285,  8229, 14068, 24440,   254, 25593,  1310,   368,   452,   247,\n",
      "          4849,   273, 32353,   285, 48393,    13,   841,   476,   564,  2366,\n",
      "           275,   247, 31579,    15,  1422,  1537,  4858,   731,   407,  2978,\n",
      "           390,  8120,   313, 16559,   780,  4632,    15,  7619,    90,   481,\n",
      "           187,   187,    26,    15,  1401, 11501,  2370, 24440,   254, 25593,\n",
      "          1198, 39978,    13, 12689,  1634,    13,   285,   643,  1355, 28234,\n",
      "            13,   247, 31579,   342, 15399,   398,   476,   320,  1077,  4217,\n",
      "           281,  1978,   841,  4957,   432,  7552, 49515,   390,  3663,    15,\n",
      "           187,   187,   740,    15,  1401, 38547,   267, 24440,   254, 25593,\n",
      "         27742,   327,   253,  7952,   368,  3153,   275,    13,   368,  1537,\n",
      "           452,   247, 31579,   323, 22952,  4957,   326,   368,   760,  8251,\n",
      "           387,  2176,  2069,   273,   253,   807,    13,   824,   347, 10831,\n",
      "         21417,   390,  5536,  8609, 14234,    15,   187,   187, 21914,   326,\n",
      "           841,   403,   816, 13991,    13,   285,   253,  1682,  9978,   323,\n",
      "           368,   588,  3469,   327,   752,  3510,   273, 14234,   368,  1211,\n",
      "           285,   849,  1199,   273,  1016,  1511,   368,   452,    15,  1310,\n",
      "           368,   452,   247,  4577, 43530,    13,   368,   778, 13398,  9050,\n",
      "           715,   247,  2014, 31579,    15, 24646,    13,   604,   368,   452,\n",
      "           247,  1781,  4849,   273,   247,  1798,  1511,   273, 14234,    13,\n",
      "           352,   778,  2430,  2709,  3812,   398,    15, 24440,   254, 15399,\n",
      "           398,   390, 37630,   476,   320,  1077,  9371,   275,  7562,  4957,\n",
      "          4858,   285,  3477,   281,  1089,  1561,  1016, 31579,    15, 50282,\n",
      "          1276,   403,  1027,  3812,   398,   309,   943,   452,   323, 10015,\n",
      "            32,   187, 21956,  3812,   398,   368,   943,   452,   323, 10015,\n",
      "          2486,   247,  7619,   254,    13,   247, 28226,  3817,    13,   247,\n",
      "         32584, 31579,    13,   247, 22756, 31579,    13,   285,   247,  1448,\n",
      "         31290, 31579,    15, 50282]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "masked_in = mask_tokens(ds[8])\n",
    "print(masked_in)\n",
    "tokenized = tokenizer(masked_in['text'], return_tensors='pt')\n",
    "print(tokenized)\n",
    "input_ids = tokenized['input_ids'].to(device='cuda')\n",
    "output = model(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 644, 50368])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 644])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 5036,  5036,  5036,  2184,  2184,  2824,  2243,  4306,  8384,  3230,\n",
       "         30714, 49502,  3230,  1426, 47826,  2184, 29409,  2243,  2243,  4306,\n",
       "         13932, 29616, 47724, 42990,  3230, 34701, 14240, 13932, 29616, 14240,\n",
       "         18369, 18369, 43716, 14240, 47724, 34701, 27730, 13070, 34701, 13932,\n",
       "         42990, 42990, 29616,  2243, 42990,   695,  4341, 11427, 11028,  4306,\n",
       "         30533, 20440,  2243,  4306, 34701, 27730, 40776,  4306, 18369,  4306,\n",
       "          2243,  4306, 29616,  2243, 29616,  2243,  2243,  4306, 18369, 18369,\n",
       "          2243, 25551, 18369,  4306,  4306,  4306,  4306, 18369, 31359, 36797,\n",
       "          4306, 18369, 34701, 47724, 28795, 13932, 34701, 29616, 15852, 47724,\n",
       "         31921, 45729, 18369,  4306,  4306, 18369, 29616, 18369,  4306, 18369,\n",
       "         29616,  2243, 29616, 27730, 34701, 29616, 11111, 29616,  4306, 21487,\n",
       "         45789, 42783,  2243,  7018,  2243,  2243,  2243,  2243,  2243, 15729,\n",
       "         49673, 18369,  2389, 47945, 14240,  6584, 37371,  4306, 13932,  6584,\n",
       "         19711, 13932, 34701,  6584, 42560,  8237,  8237, 29616, 42287,  4920,\n",
       "         23642,  1111, 47945, 10422, 30345, 11609,  1569, 27730, 40351, 29616,\n",
       "          4306,  2243,  4306, 29616,  4306, 29616,  2243,   303, 28795,  6712,\n",
       "          4306,  2243, 29616,  2243,  6984,  4306, 18369, 14240,  2243, 29616,\n",
       "          4306,  3230,  2243,  4306, 21384,  4306,  1626, 19916,  2243, 32061,\n",
       "         35160, 34701, 19236, 30197, 11617, 13932,  2243,  4306,  4306,  2243,\n",
       "         27730, 36974,  4306,  4306, 34701,  4306,  6565, 14240, 14240,  4306,\n",
       "          2243, 13658,  2243, 29616,  3230, 31104, 33643,  6565,    13,  4306,\n",
       "         47471,  7036, 20787, 28795,  2184, 10090, 14240,  3165,  4306, 23642,\n",
       "         19311,  1460, 25765,  6565,  6565, 14240, 12501,  8031,  4306, 13932,\n",
       "         36626, 27206,  4306,  4306,  2184, 36626,  4306, 27730,  7204, 34701,\n",
       "          4306, 34701,  6696,  6696,  2243, 29493, 45789, 19576, 15852,  2243,\n",
       "          2243, 49502, 29616,  4349, 47826, 49673, 28850, 15642, 15852, 26791,\n",
       "          5811, 13932, 19236, 36626, 41125, 31656,  2815, 36626, 16586, 16586,\n",
       "          4917, 49228,  7677,  5563, 23104, 28795, 27730, 40351, 29616,  4306,\n",
       "          2243, 29616,  2243,  4306,  6555,  8979, 37371, 37371,  2243,  2243,\n",
       "         11180,  2243, 37371, 38104, 40513, 13787,  2885, 19236, 25765, 15476,\n",
       "         29662,  2419, 29616,  8624, 17638, 10598,  3165,  5333, 37489, 34416,\n",
       "          2866, 19384,  6478,  1059,  3165, 33756, 29662,  5692, 47240, 43290,\n",
       "         17920,  1059, 33659,  9513,   675, 15053,  6983, 27730, 30388, 34701,\n",
       "         29616, 18369, 29616, 11362, 29616, 19576,  8237,  2243, 11362,  4306,\n",
       "          2243,  2243, 50151, 48309, 29616,  2243, 14240,  3138,  4306,  1450,\n",
       "          1460, 45674,  8467, 17920, 13932, 26987, 30147, 49502, 11873,  3165,\n",
       "         45570, 24886, 14381, 13932, 29616,  5033, 17920,  4045, 48041, 43716,\n",
       "         13932, 13932, 27730, 49442, 34701,  4306, 34701, 18369, 19576,  6696,\n",
       "          2243, 29616,  3230,  2243, 29616, 49502, 24244, 25046, 27666, 35942,\n",
       "         38742, 10390,  4306,  9644,   915,  2823, 41125, 14240, 19667,  2496,\n",
       "          7036,  1379,  3165, 23642, 23831,  3611,  1111, 20046, 14240, 29616,\n",
       "         20440,  4306,  2243, 13932, 29616, 14240, 18369,  4306,  4306, 19657,\n",
       "         27730,  2897, 29616, 34701, 29616,  8625,  4306,  2243, 29616, 33507,\n",
       "         50281,  7782, 36626, 37371,  2496, 19236,  6056, 24507, 16586, 10478,\n",
       "          6056, 14705,  4306,  2243, 17699,  4306, 47098,  7036,  3522, 32754,\n",
       "         10390, 46107,  6097, 32182, 22850,  7677, 18617,  3718,  5432, 27730,\n",
       "         45528, 25765,  4306,   376, 29616, 29616,  6696,  2243, 29616, 49502,\n",
       "           752,  1059,  9359, 32163, 34701,  2839, 34170, 21706, 19236, 11491,\n",
       "         25765,  1964,  4306, 18369, 29616, 28863, 31656, 13932,  5742,  2389,\n",
       "         19758, 20891,    13, 18236, 23801, 29616, 10390, 30714, 19758,  4306,\n",
       "         13932, 34841,  4306,  4306, 13932, 27730, 24489,  7455,   942, 10390,\n",
       "         19248, 42990, 32482,  4306, 16037, 39036, 40776, 14240, 21695,  5811,\n",
       "            15,  8120,  4023,  3088, 37712,  1059,  1555, 16484, 20950,  7947,\n",
       "         10390, 25094, 15591, 10390, 12633, 49033, 25057, 16484, 21560, 13653,\n",
       "          1912, 26219, 22502,  3088,  1555, 38104, 10390, 11411, 38104,  1059,\n",
       "          9027, 30740, 10532,  3165, 45528, 13184, 41767,  7036, 15331,  7036,\n",
       "         38818, 32353,  1555, 40632, 12511, 10532, 28613, 24284,  7221, 22502,\n",
       "         33382,  7070, 11817,  4502,  4306,  4306, 49447, 37371, 37371, 36613,\n",
       "          4306,  7180, 15563, 15331, 25967, 20041,  9258, 41851, 31451, 32182,\n",
       "         22375, 41482, 45528, 41851, 10090, 19657,   971, 37371, 27730, 36974,\n",
       "          5070,  2184,  6555,  3230,  4306, 21307, 29616, 19422, 30144,  3230,\n",
       "           819,  9038,  2243,  3230, 37371, 25308,  2389,  5993, 30144, 15563,\n",
       "          7455,  2184,  2243, 37371,  6584, 15729,  4225, 15852, 25765, 15729,\n",
       "          4306,  3165,  6584,  2243,   303,  2243, 27730,  6696,  6696,  3165,\n",
       "          2269,  3165,  2243,  2187]], device='cuda:0')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = torch.argmax(output.logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'echechechgogoips momancynextanaoccur Hoeanairc hitchgosci mom momancyuple muminche fermentana Mum circlesuple mum circles neglect neglect velvet circlesinche Mumolerancehelp Mumuple ferment ferment mum mom fermentricocationsviryancy Owumm momancy Mumolerance Hollowancy neglectancy momancy mum mom mum mom momancy neglect neglect mom yoga neglectancyancyancyancy neglect neighbours Mutualancy neglect Mumincheeniumuple Mum mumParentinche fuzzy Hammond neglectancyancy neglect mum neglectancy neglect mum mom mumolerance Mum mum shr mumancy toilet textbooks Eg mom sup mom mom mom mom mom hug Ai neglect love Aviv circlesvioorsancyupleviAgeuple Mumviammers Virgin Virgin mumaways mortliv mat Avivauxplaces dish lawolerance pits mumancy momancy mumancy mum momimeniumweightancy mom mum mom Madancy neglect circles mom mumancyana momancyuruancyury praise mom pulp Gor Mumangles yog deeplyuple momancyancy momolerance Beautyancyancy Mumancyords circles circlesancy mom Specifically mom mumana parenting localityords,ancy allergies concungeeniumgo laid circlesatalancyliv wit purcentralordsords circlesPut stackancyupleudsunctureancyancygoudsancyolerance jour Mumancy Mum Hall Hall mom nonprofit textbooksHelpParent mom mom Hoe mumathe hitch AisleepatarParent flavorsowsupleanglesuds thru donatelimudsorderedordered yieldifold stret mig bendeniumolerance pits mumancy mom mum momancyeric shopoorsoors mom momurd momoors Thingescent nucleus parentanglescentralgridutta house mum principaligsonymousatal shiftropolisrequently…irs elevletatal scalputtarientbeth Packersviewsletdent beginsreatighters recallolerance inclination Mum mum neglect mum Age mumHelp Virgin mom Ageancy mom mom spiritualityuca mum mom circles![ancy:: pur Orient skyviewsuple loversopia Hoe lectatalsburgumblingimsuple mumicitviewsDo slam velvetupleupleolerancearda Mumancy Mum neglectHelp Hall mom mumana mom mum Hoe MiniCampaines HundredLET honorancy lean suppcomm thru circlesencerude conc takeatallivuffleague mat acres circles mumummancy momuple mum circles neglectancyancycontrolerance ang mum Mum mumvolancy mom mumnuts[CLS]iveredudsoorsudeangleslandsbuckordered headerlandspieceancy mom bayancy meats conc dateento honor gladly′strilights stret� favoritzolerance!’centralancyra mum mum Hall mom mum Hoe whatlet sod lump Mum rat riot Hillsangles Kingdomcentral promancy neglect mumcult donateuple specifically loveabcenta, practically Honor mum honoroccurabcancyuple Heightsancyancyupleolerancehallberedments honor honour ferment postersancy sneowe Hollow circles Helpows. occasion balldolaidlet treatcoverroerior honormoment\\\\/ honor folks/@ leveragecover Treat odds wonsever hersdo treat Thing honor tracks Thinglet.’ favorites revolutionatal!’ suicide Levels conc toss conc lbs shorts treat Fit pounds revolution RD hurry minim herswonipljections tumorancyancyDefaultsoorsoorsbratesancy tables contra tossabin scratchinson (& wontstri reflex shakes!’ (& laidcontr wantoorsolerance Beautyplacegoericanaancy Plant mum napgrownana pricate momanaoors jam loveBLgrown contraberedgo momoorsvi hugotedParentcentral hugancyatalvi momim momolerance Hall Hallatalercatal momempt'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(preds[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
