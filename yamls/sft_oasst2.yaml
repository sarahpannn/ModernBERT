# Configuration for SFT training on OASST2 dataset
# Usage: python sft_oasst2.py yamls/sft_oasst2.yaml

# Run settings
run_name: "sft_oasst2_modern_bert"
seed: 42

# Model configuration
model:
  name: modern_bert  # or flex_bert, hf_bert
  pretrained_model_name: "answerdotai/ModernBERT-large"  # or your preferred model
  use_pretrained: true
  tokenizer_name: "answerdotai/ModernBERT-large"
  gradient_checkpointing: false

  model_config:
    attention_bias: false
    attention_dropout: 0.0
    bos_token_id: 50281
    classifier_activation: gelu
    classifier_bias: false
    classifier_dropout: 0.0
    classifier_pooling: mean
    cls_token_id: 50281
    decoder_bias: true
    deterministic_flash_attn: false
    embedding_dropout: 0.0
    eos_token_id: 50282
    global_attn_every_n_layers: 3
    global_rope_theta: 160000.0
    gradient_checkpointing: false
    hidden_activation: gelu
    hidden_size: 1024
    hidden_dropout_prob: 0.0
    initializer_cutoff_factor: 2.0
    initializer_range: 0.02
    intermediate_size: 2624
    layer_norm_eps: 1e-05
    local_attention: 128
    local_rope_theta: 10000.0
    max_position_embeddings: 8192
    mlp_bias: false
    mlp_dropout: 0.0
    model_type: modernbert
    norm_bias: false
    norm_eps: 1e-05
    num_attention_heads: 16
    num_hidden_layers: 28
    pad_token_id: 50283
    position_embedding_type: absolute
    sep_token_id: 50282
    tie_word_embeddings: true
    torch_dtype: float32
    transformers_version: 4.47.0.dev0
    vocab_size: 50368
    final_norm: true
    sliding_window: 128
    loss_function: fa_cross_entropy
  
# Training data loader
train_loader:
  tokenizer_name: "answerdotai/ModernBERT-large"
  split: "train"
  max_seq_len: 512
  num_workers: 4
  shuffle: true
  drop_last: true
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true

# Optional evaluation data loader  
# eval_loader:
#   tokenizer_name: "answerdotai/ModernBERT-base"
#   split: "validation"
#   max_seq_len: 512
#   num_workers: 4
#   shuffle: false
#   drop_last: false

# Batch size configuration
global_train_batch_size: 64
device_train_microbatch_size: 4
# global_eval_batch_size: 32
# device_eval_microbatch_size: 4

# Training duration and evaluation
max_duration: "3ep"  # 3 epochs
eval_interval: "500ba"  # evaluate every 500 batches
train_subset_num_batches: -1  # use full dataset
eval_subset_num_batches: -1

# Optimizer configuration
optimizer:
  name: adamw
  lr: 5.0e-5
  betas: [0.9, 0.999]
  eps: 1.0e-8
  weight_decay: 0.01
  filter_bias_norm_wd: true

# Learning rate scheduler
scheduler:
  name: linear_decay_with_warmup
  t_warmup: "0.1dur"  # 10% warmup
  alpha_f: 0.0  # decay to 0

# Precision and device settings
precision: "amp_fp16"  # mixed precision training
device: "gpu"

# Checkpointing
save_folder: "./checkpoints/sft_oasst2"
save_interval: "1000ba"
save_num_checkpoints_to_keep: 2
save_overwrite: false
save_final_model: true
final_model_name: "ModernBERT-sft-oasst2"

# Logging
progress_bar: true
log_to_console: false
console_log_interval: "10ba"

# Loggers (optional - uncomment to use)
loggers:
  wandb:
    project: "modernbert-sft"
    name: "oasst2-sft"

# Callbacks for monitoring
callbacks:
  speed_monitor:
    window_size: 10
  lr_monitor: {}
  memory_monitor: {}
  # optimizer_monitor:
  #   log_optimizer_metrics: true