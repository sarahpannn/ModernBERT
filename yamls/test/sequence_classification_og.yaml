# Use this YAML to verify that fine-tuning starter script works. Runs on CPU or GPUs (if available).
# From `examples/bert`, run:
#   `composer sequence_classification.py yamls/test/sequence_classification.yaml` (HuggingFace BERT)
#   `composer sequence_classification.py yamls/test/sequence_classification.yaml model.name=mosaic_bert` (Mosaic BERT)

max_seq_len: 3600
tokenizer_name: bclavie/olmo_bert_template

# Run Name
run_name: ${model.freeze_layers}_layers_frozen_${optimizer.lr}_lr
checkpoint_run_name: "experiment_001" 

# load_path: # (Optionally) provide a composer checkpoint to use for the starting weights
# Model
model:
  name: flex_bert
  num_labels: 2
  pretrained_model_name: answerdotai/ModernBERT-large
  tokenizer_name: ${tokenizer_name}
  disable_train_metrics: true
  # pretrained_checkpoint: /home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/correct_names.pt
  pretrained_checkpoint: answerdotai/ModernBERT-large
  # pretrained_checkpoint: /home/public/span/MATH_DPO/modern_bert_test/bert24/opt/home/bert24/DPR/output/bert24-large-v2-learning-rate-decay-v3-50B-ep0-ba9000-rank0/bert24-large-v2-learning-rate-decay-v3-50B-ep0-ba9000-rank0-colbertlr-0.0001-harddataset/checkpoint-305/model.pth
  # pretrained_checkpoint: /home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/ep3-ba1626-rank0.pt
  # pretrained_checkpoint: /home/public/span/MATH_DPO/modern_bert_test/bert24/ep1-ba21195-rank0.pt
  freeze: true
  # freeze_layers: 27 # valid are 0 through 27
  
  model_config:
    attention_bias: false
    attention_dropout: 0.0
    bos_token_id: 50281
    classifier_activation: gelu
    classifier_bias: false
    classifier_dropout: 0.0
    classifier_pooling: mean
    cls_token_id: 50281
    decoder_bias: true
    deterministic_flash_attn: false
    embedding_dropout: 0.0
    eos_token_id: 50282
    global_attn_every_n_layers: 3
    global_rope_theta: 160000.0
    gradient_checkpointing: false
    hidden_activation: gelu
    hidden_size: 1024
    # hidden_dropout_prob: 0.55
    hidden_dropout_prob: 0.0
    initializer_cutoff_factor: 2.0
    initializer_range: 0.02
    intermediate_size: 2624
    layer_norm_eps: 1e-05
    local_attention: 128
    local_rope_theta: 10000.0
    max_position_embeddings: 8192
    mlp_bias: false
    mlp_dropout: 0.0
    model_type: modernbert
    norm_bias: false
    norm_eps: 1e-05
    num_attention_heads: 16
    num_hidden_layers: 28
    pad_token_id: 50283
    position_embedding_type: absolute
    sep_token_id: 50282
    tie_word_embeddings: true
    torch_dtype: float32
    transformers_version: 4.47.0.dev0
    vocab_size: 50368
    final_norm: true
    sliding_window: 128
    loss_function: fa_cross_entropy

# Dataloaders
train_loader:
  split: train
  tokenizer_name: ${tokenizer_name}
  max_seq_len: ${max_seq_len}
  shuffle: true
  drop_last: true
  num_workers: 8

eval_loader:
  split: train
  tokenizer_name: ${tokenizer_name}
  max_seq_len: ${max_seq_len}
  shuffle: true
  drop_last: true
  num_workers: 8

# Optimization
scheduler:
  name: linear_decay_with_warmup
  # name: cosine_with_warmup
  t_warmup: 0.06dur # Warmup to the full LR for 6% of the training duration
  alpha_f: 0.02 # Linearly decay to 0.02x the full LR by the end of the training duration

optimizer:
  name: decoupled_adamw
  # lr: 1e-4
  betas:
  - 0.9
  - 0.98
  eps: 1.0e-06
  weight_decay: 1.0e-6

# Training duration and evaluation frequency
max_duration: 1ep
eval_interval: 50ba
eval_subset_num_batches: 4 # For code testing, evaluate on a subset of 4 batches
global_train_batch_size: 256

# System
seed: 17
device_eval_microbatch_size: 8
device_train_microbatch_size: 8
precision: fp32

# Logging
progress_bar: false
log_to_console: false
console_log_interval: 1ba

loggers:
  wandb:
    project: bert24-runs-cls
    entity: sarahpan888

callbacks:
  speed_monitor:
    window_size: 5
  lr_monitor: {}

# save_folder: /home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints
# save_fs