# Use this YAML to verify that fine-tuning starter script works. Runs on CPU or GPUs (if available).
# From `examples/bert`, run:
#   `composer sequence_classification.py yamls/test/sequence_classification.yaml` (HuggingFace BERT)
#   `composer sequence_classification.py yamls/test/sequence_classification.yaml model.name=mosaic_bert` (Mosaic BERT)

max_seq_len: 1024
tokenizer_name: bclavie/olmo_bert_template

# Run Name
run_name: "reward_bench_db"
checkpoint_run_name: "experiment_001" 

load_path: # (Optionally) provide a composer checkpoint to use for the starting weights

# Model
model:
  name: flex_bert
  num_labels: 2
  pretrained_model_name: bert-base-uncased
  tokenizer_name: ${tokenizer_name}
  disable_train_metrics: true
  pretrained_checkpoint: /home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints/ep1-ba1000-rank0.pt
  # pretrained_checkpoint: /home/public/span/MATH_DPO/modern_bert_test/bert24/ep1-ba21195-rank0.pt
  
  model_config:
    vocab_size: 50368
    init_method: full_megatron
    num_hidden_layers: 22
    hidden_size: 768
    intermediate_size: 1152
    num_attention_heads: 12 # to have head size of 64
    attention_layer: rope
    attention_probs_dropout_prob: 0.0
    attn_out_bias: false
    attn_out_dropout_prob: 0.1
    attn_qkv_bias: false
    bert_layer: prenorm
    embed_dropout_prob: 0.0
    embed_norm: true
    final_norm: true
    skip_first_prenorm: true
    embedding_layer: sans_pos
    loss_function: fa_cross_entropy
    loss_kwargs:
      reduction: mean
    mlp_dropout_prob: 0.0
    mlp_in_bias: false
    mlp_layer: glu
    mlp_out_bias: false
    normalization: layernorm
    norm_kwargs:
      eps: 1e-5
      bias: false
    hidden_act: gelu
    head_pred_act: gelu
    activation_function: gelu # better safe than sorry
    padding: unpadded
    sparse_prediction: false
    rotary_emb_dim: null
    rotary_emb_base: 10000.0
    rotary_emb_scale_base: null
    rotary_emb_interleaved: false
    allow_embedding_resizing: true
    sliding_window: 128
    global_attn_every_n_layers: 3
    unpad_embeddings: true
    compile_model: true
    masked_prediction: true

# Dataloaders

eval_loader:
  split: train
  tokenizer_name: ${tokenizer_name}
  max_seq_len: ${max_seq_len}
  shuffle: true
  drop_last: true
  num_workers: 8

# Training duration and evaluation frequency
max_duration: 3ep
eval_interval: 50ba
# eval_subset_num_batches: 4 # For code testing, evaluate on a subset of 4 batches
global_train_batch_size: 128

# System
seed: 17
device_eval_microbatch_size: 1
device_train_microbatch_size: 32
precision: fp32

# Logging
progress_bar: false
log_to_console: false
console_log_interval: 1ba

# loggers:
#   wandb:
#     project: reward_model
#     entity: sarahpan888

callbacks:
  speed_monitor:
    window_size: 5
  lr_monitor: {}

save_folder: /home/public/span/MATH_DPO/modern_bert_test/bert24/checkpoints
save_filename: helpsteer-2-pref